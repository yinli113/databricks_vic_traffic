{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ffe0aa-7ef3-4ae8-baf7-84df83d022f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./1_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81024517-c083-4230-81bc-aa0355e0fd42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, importlib\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"ENV\", \"dev\", [\"dev\", \"qa\"], \"Environment\")\n",
    "    dbutils.widgets.dropdown(\"STORAGE_ACCOUNT\", \"trafficsa2\", [\"trafficsa2\", \"trafficsaqa\"], \"Storage account\")\n",
    "    dbutils.widgets.text(\"METASTORE_ACCOUNT\", \"trafficsa2\", \"Metastore account\")\n",
    "    os.environ[\"ENV\"] = dbutils.widgets.get(\"ENV\").strip().lower()\n",
    "    os.environ[\"STORAGE_ACCOUNT\"] = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\n",
    "    os.environ[\"METASTORE_ACCOUNT\"] = (dbutils.widgets.get(\"METASTORE_ACCOUNT\") or os.environ[\"STORAGE_ACCOUNT\"]).strip()\n",
    "except NameError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "913a3c29-f106-4bd9-924c-bff96a05b942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, year, month, upper, trim, coalesce, lit,\n",
    "    sum as sum_, count as count_, min as min_, max as max_\n",
    ")\n",
    "\n",
    "DEV_MODE   = True                    # dev = allow precreate overwrite; prod = set False\n",
    "THRESHOLDS = {\"low\": 40, \"med\": 75, \"high\": 110}\n",
    "CDF_FROM_VERSION   = \"0\"             # or set startingTimestamp instead (e.g. \"2025-01-01T00:00:00Z\")\n",
    "\n",
    "def _bootstrap_uc(spark: SparkSession, catalog: str, db_name: str) -> None:\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db_name}\")\n",
    "    spark.sql(f\"USE CATALOG {catalog}\")\n",
    "    spark.sql(f\"USE {db_name}\")\n",
    "\n",
    "def _norm_nm_region_expr(c: str) -> F.Column:\n",
    "    return upper(trim(col(c)))\n",
    "\n",
    "def resolve_silver_fqn(conf: Config, explicit: Optional[str] = None) -> str:\n",
    "    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "    catalog, db = conf.catalog, conf.db_name\n",
    "    candidates: List[str] = []\n",
    "    if explicit: candidates.append(explicit)\n",
    "    candidates += [\n",
    "        conf.table_fqn(conf.silver_table),              # canonical (traffic_silver_events)\n",
    "        f\"{catalog}.{db}.traffic_silver_events\",\n",
    "        f\"{catalog}.{db}.traffic_silver_envents\",       # common typo\n",
    "        f\"{catalog}.{db}.traffic_silver_fact\",          # legacy\n",
    "        f\"{catalog}.{db}.traffic_silver_event\",         # singular\n",
    "    ]\n",
    "    tried = set()\n",
    "    for fqn in candidates:\n",
    "        if fqn in tried: \n",
    "            continue\n",
    "        tried.add(fqn)\n",
    "        try:\n",
    "            spark.table(fqn).limit(1).count()\n",
    "            print(f\"✅ Using Silver source: {fqn}\")\n",
    "            return fqn\n",
    "        except AnalysisException:\n",
    "            pass\n",
    "    print(\"❌ Could not find a Silver table. Tried:\", sorted(tried))\n",
    "    spark.sql(f\"SHOW TABLES IN {catalog}.{db} LIKE 'traffic_silver*'\").show(truncate=False)\n",
    "    raise RuntimeError(\"Silver table not found. Align your config or rename the table.\")\n",
    "\n",
    "def ensure_region_lookup_coverage(\n",
    "    spark: SparkSession,\n",
    "    silver_fact_fqn: str,\n",
    "    region_lookup_fqn: str,\n",
    "    backfill_unknown: bool = True\n",
    ") -> None:\n",
    "    try:\n",
    "        spark.table(region_lookup_fqn).limit(1).count()\n",
    "    except AnalysisException:\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {region_lookup_fqn} (\n",
    "                NM_REGION STRING,\n",
    "                SUBURB   STRING\n",
    "            )\n",
    "            USING DELTA\n",
    "        \"\"\")\n",
    "    lk_norm = (spark.table(region_lookup_fqn)\n",
    "        .select(_norm_nm_region_expr(\"NM_REGION\").alias(\"NM_REGION\"), col(\"SUBURB\"))\n",
    "        .dropDuplicates([\"NM_REGION\"]))\n",
    "    silver_regions = (spark.table(silver_fact_fqn)\n",
    "        .select(_norm_nm_region_expr(\"NM_REGION\").alias(\"NM_REGION\"))\n",
    "        .distinct())\n",
    "    missing = silver_regions.join(lk_norm.select(\"NM_REGION\"), \"NM_REGION\", \"left_anti\")\n",
    "    miss_cnt = missing.count()\n",
    "    if miss_cnt > 0 and backfill_unknown:\n",
    "        (missing.withColumn(\"SUBURB\", lit(\"Unknown\"))\n",
    "                .write.format(\"delta\").mode(\"append\").saveAsTable(region_lookup_fqn))\n",
    "        print(f\"✅ Backfilled {miss_cnt} missing NM_REGION in region_lookup as 'Unknown'\")\n",
    "    # normalize & dedupe table\n",
    "    lk_final = (spark.table(region_lookup_fqn)\n",
    "        .select(_norm_nm_region_expr(\"NM_REGION\").alias(\"NM_REGION\"), col(\"SUBURB\"))\n",
    "        .dropDuplicates([\"NM_REGION\"]))\n",
    "    lk_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").saveAsTable(region_lookup_fqn)\n",
    "\n",
    "# --------- create empty targets (first time) ----------\n",
    "def _create_or_overwrite_empty(df0: DataFrame, table_fqn: str, dev_mode: bool) -> None:\n",
    "    spark = df0.sparkSession\n",
    "    if dev_mode:\n",
    "        df0.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(table_fqn)\n",
    "    else:\n",
    "        try:\n",
    "            spark.table(table_fqn).limit(1).count()\n",
    "        except AnalysisException:\n",
    "            df0.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(table_fqn)\n",
    "\n",
    "def precreate_gold_tables(\n",
    "    spark: SparkSession,\n",
    "    silver_fact_fqn: str,\n",
    "    region_lookup_fqn: str,\n",
    "    t_region_hourly: str,\n",
    "    t_detector_hourly: str,\n",
    "    t_region_monthly: str,\n",
    "    t_detector_cong: str,\n",
    "    dev_mode: bool\n",
    ") -> None:\n",
    "    src0 = spark.table(silver_fact_fqn).limit(0)\n",
    "    fact0 = src0.select(\n",
    "        _norm_nm_region_expr(\"NM_REGION\").alias(\"NM_REGION\"),\n",
    "        \"NB_SCATS_SITE\", \"NB_DETECTOR\", \"Interval_EndTime\",\n",
    "        coalesce(col(\"Volume\").cast(\"long\"), lit(0)).alias(\"Volume\")\n",
    "    )\n",
    "    lk0 = (spark.table(region_lookup_fqn)\n",
    "            .select(_norm_nm_region_expr(\"NM_REGION\").alias(\"NM_REGION\"), col(\"SUBURB\"))\n",
    "            .dropDuplicates([\"NM_REGION\"]))\n",
    "    df0 = (fact0.join(lk0, [\"NM_REGION\"], \"left\")\n",
    "                 .withColumn(\"SUBURB\", coalesce(col(\"SUBURB\"), lit(\"Unknown\"))))\n",
    "    detector_hourly0 = (df0.groupBy(\"NM_REGION\",\"SUBURB\",\"NB_SCATS_SITE\",\"NB_DETECTOR\",\"Interval_EndTime\")\n",
    "                          .agg(F.sum(\"Volume\").alias(\"Hourly_Volume\")))\n",
    "    region_hourly0   = (detector_hourly0.groupBy(\"NM_REGION\",\"SUBURB\",\"Interval_EndTime\")\n",
    "                                         .agg(F.sum(\"Hourly_Volume\").alias(\"Hourly_Volume\")))\n",
    "    region_monthly0  = (region_hourly0.withColumn(\"Year\", F.year(\"Interval_EndTime\"))\n",
    "                                        .withColumn(\"Month\", F.month(\"Interval_EndTime\"))\n",
    "                                        .groupBy(\"NM_REGION\",\"SUBURB\",\"Year\",\"Month\")\n",
    "                                        .agg(F.sum(\"Hourly_Volume\").alias(\"Monthly_Volume\")))\n",
    "    detector_cong0   = (detector_hourly0\n",
    "                        .withColumn(\"Congestion_Flag\", F.lit(False))\n",
    "                        .withColumn(\"Congestion_Level\", F.lit(\"Normal\")))\n",
    "    _create_or_overwrite_empty(region_hourly0,   t_region_hourly,   dev_mode)\n",
    "    _create_or_overwrite_empty(detector_hourly0, t_detector_hourly, dev_mode)\n",
    "    _create_or_overwrite_empty(region_monthly0,  t_region_monthly,  dev_mode)\n",
    "    _create_or_overwrite_empty(detector_cong0,   t_detector_cong,   dev_mode)\n",
    "    print(\"✅ Pre-created Gold tables (empty schemas).\")\n",
    "\n",
    "# --------- MERGE helpers ----------\n",
    "def _merge_detector_hourly(spark: SparkSession, df: DataFrame, target: str):\n",
    "    df.createOrReplaceTempView(\"upd_dh\")\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {target} AS t\n",
    "    USING upd_dh AS s\n",
    "    ON  t.NM_REGION = s.NM_REGION\n",
    "    AND t.SUBURB = s.SUBURB\n",
    "    AND t.NB_SCATS_SITE = s.NB_SCATS_SITE\n",
    "    AND t.NB_DETECTOR = s.NB_DETECTOR\n",
    "    AND t.Interval_EndTime = s.Interval_EndTime\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "      t.Hourly_Volume = s.Hourly_Volume\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "      NM_REGION, SUBURB, NB_SCATS_SITE, NB_DETECTOR, Interval_EndTime, Hourly_Volume\n",
    "    ) VALUES (\n",
    "      s.NM_REGION, s.SUBURB, s.NB_SCATS_SITE, s.NB_DETECTOR, s.Interval_EndTime, s.Hourly_Volume\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "def _merge_region_hourly(spark: SparkSession, df: DataFrame, target: str):\n",
    "    df.createOrReplaceTempView(\"upd_rh\")\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {target} AS t\n",
    "    USING upd_rh AS s\n",
    "    ON  t.NM_REGION = s.NM_REGION\n",
    "    AND t.SUBURB = s.SUBURB\n",
    "    AND t.Interval_EndTime = s.Interval_EndTime\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "      t.Hourly_Volume = s.Hourly_Volume\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "      NM_REGION, SUBURB, Interval_EndTime, Hourly_Volume\n",
    "    ) VALUES (\n",
    "      s.NM_REGION, s.SUBURB, s.Interval_EndTime, s.Hourly_Volume\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "def _merge_region_monthly(spark: SparkSession, df: DataFrame, target: str):\n",
    "    df.createOrReplaceTempView(\"upd_rm\")\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {target} AS t\n",
    "    USING upd_rm AS s\n",
    "    ON  t.NM_REGION = s.NM_REGION\n",
    "    AND t.SUBURB = s.SUBURB\n",
    "    AND t.Year = s.Year\n",
    "    AND t.Month = s.Month\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "      t.Monthly_Volume = s.Monthly_Volume\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "      NM_REGION, SUBURB, Year, Month, Monthly_Volume\n",
    "    ) VALUES (\n",
    "      s.NM_REGION, s.SUBURB, s.Year, s.Month, s.Monthly_Volume\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "def _merge_detector_congestion(spark: SparkSession, df: DataFrame, target: str):\n",
    "    df.createOrReplaceTempView(\"upd_dc\")\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {target} AS t\n",
    "    USING upd_dc AS s\n",
    "    ON  t.NM_REGION = s.NM_REGION\n",
    "    AND t.SUBURB = s.SUBURB\n",
    "    AND t.NB_SCATS_SITE = s.NB_SCATS_SITE\n",
    "    AND t.NB_DETECTOR = s.NB_DETECTOR\n",
    "    AND t.Interval_EndTime = s.Interval_EndTime\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "      t.Hourly_Volume = s.Hourly_Volume,\n",
    "      t.Congestion_Flag = s.Congestion_Flag,\n",
    "      t.Congestion_Level = s.Congestion_Level\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "      NM_REGION, SUBURB, NB_SCATS_SITE, NB_DETECTOR, Interval_EndTime, Hourly_Volume, Congestion_Flag, Congestion_Level\n",
    "    ) VALUES (\n",
    "      s.NM_REGION, s.SUBURB, s.NB_SCATS_SITE, s.NB_DETECTOR, s.Interval_EndTime, s.Hourly_Volume, s.Congestion_Flag, s.Congestion_Level\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "# --------- foreachBatch using CDF (correctness) ----------\n",
    "def make_foreach_batch_cdf_merge(\n",
    "    silver_fact_fqn: str,\n",
    "    region_lookup_fqn: str,\n",
    "    thresholds: dict,\n",
    "    t_region_hourly: str,\n",
    "    t_detector_hourly: str,\n",
    "    t_region_monthly: str,\n",
    "    t_detector_cong: str\n",
    "):\n",
    "    low_t, med_t, high_t = thresholds[\"low\"], thresholds[\"med\"], thresholds[\"high\"]\n",
    "\n",
    "    def foreach_batch(batch_df: DataFrame, batch_id: int):\n",
    "        if batch_df.isEmpty():\n",
    "            print(f\"Batch {batch_id}: no changes.\")\n",
    "            return\n",
    "        spark = batch_df.sparkSession\n",
    "\n",
    "        # 1) Identify impacted keys from CDF batch (normalized region)\n",
    "        changed = (batch_df\n",
    "            .select(\n",
    "                _norm_nm_region_expr(\"NM_REGION\").alias(\"NM_REGION\"),\n",
    "                \"NB_SCATS_SITE\",\"NB_DETECTOR\",\"Interval_EndTime\"\n",
    "            )\n",
    "            .dropDuplicates()\n",
    "        )\n",
    "        keys_rh = changed.select(\"NM_REGION\",\"Interval_EndTime\").distinct()\n",
    "        keys_rm = (keys_rh\n",
    "            .withColumn(\"Year\", F.year(\"Interval_EndTime\"))\n",
    "            .withColumn(\"Month\", F.month(\"Interval_EndTime\"))\n",
    "            .select(\"NM_REGION\",\"Year\",\"Month\").distinct())\n",
    "\n",
    "        # 2) Pull the latest snapshot for JUST those keys from Silver (ground truth)\n",
    "        silver = spark.table(silver_fact_fqn).select(\n",
    "            _norm_nm_region_expr(\"NM_REGION\").alias(\"NM_REGION\"),\n",
    "            \"NB_SCATS_SITE\",\"NB_DETECTOR\",\"Interval_EndTime\",\n",
    "            coalesce(col(\"Volume\").cast(\"long\"), lit(0)).alias(\"Volume\")\n",
    "        )\n",
    "\n",
    "        lk = (spark.table(region_lookup_fqn)\n",
    "                .select(_norm_nm_region_expr(\"NM_REGION\").alias(\"NM_REGION\"), col(\"SUBURB\"))\n",
    "                .dropDuplicates([\"NM_REGION\"]))\n",
    "\n",
    "        # 3) Detector-hour (exact latest values)\n",
    "        det_imp = (silver.join(changed, [\"NM_REGION\",\"NB_SCATS_SITE\",\"NB_DETECTOR\",\"Interval_EndTime\"], \"inner\")\n",
    "                         .join(lk, [\"NM_REGION\"], \"left\")\n",
    "                         .withColumn(\"SUBURB\", coalesce(col(\"SUBURB\"), lit(\"Unknown\")))\n",
    "                         .groupBy(\"NM_REGION\",\"SUBURB\",\"NB_SCATS_SITE\",\"NB_DETECTOR\",\"Interval_EndTime\")\n",
    "                         .agg(F.sum(\"Volume\").alias(\"Hourly_Volume\")))\n",
    "\n",
    "        # 4) Region-hour (recompute from snapshot for impacted keys)\n",
    "        region_hourly = (silver.join(keys_rh, [\"NM_REGION\",\"Interval_EndTime\"], \"inner\")\n",
    "                               .groupBy(\"NM_REGION\",\"Interval_EndTime\").agg(F.sum(\"Volume\").alias(\"Hourly_Volume\"))\n",
    "                               .join(lk, [\"NM_REGION\"], \"left\")\n",
    "                               .withColumn(\"SUBURB\", coalesce(col(\"SUBURB\"), lit(\"Unknown\")))\n",
    "                               .select(\"NM_REGION\",\"SUBURB\",\"Interval_EndTime\",\"Hourly_Volume\"))\n",
    "\n",
    "        # 5) Region-month (recompute from snapshot for impacted months)\n",
    "        region_monthly = (silver\n",
    "            .withColumn(\"Year\", F.year(\"Interval_EndTime\"))\n",
    "            .withColumn(\"Month\", F.month(\"Interval_EndTime\"))\n",
    "            .join(keys_rm, [\"NM_REGION\",\"Year\",\"Month\"], \"inner\")\n",
    "            .groupBy(\"NM_REGION\",\"Year\",\"Month\").agg(F.sum(\"Volume\").alias(\"Monthly_Volume\"))\n",
    "            .join(lk, [\"NM_REGION\"], \"left\")\n",
    "            .withColumn(\"SUBURB\", coalesce(col(\"SUBURB\"), lit(\"Unknown\")))\n",
    "            .select(\"NM_REGION\",\"SUBURB\",\"Year\",\"Month\",\"Monthly_Volume\"))\n",
    "\n",
    "        # 6) Detector congestion (derived from detector_hourly)\n",
    "        detector_congestion = (det_imp\n",
    "            .withColumn(\"Congestion_Flag\", F.when(col(\"Hourly_Volume\") > low_t, True).otherwise(False))\n",
    "            .withColumn(\"Congestion_Level\",\n",
    "                F.when(col(\"Hourly_Volume\") > high_t, \"High\")\n",
    "                 .when(col(\"Hourly_Volume\") > med_t, \"Medium\")\n",
    "                 .when(col(\"Hourly_Volume\") > low_t, \"Low\")\n",
    "                 .otherwise(\"Normal\"))\n",
    "        )\n",
    "\n",
    "        # 7) MERGE into targets (upsert)\n",
    "        _merge_detector_hourly(spark, det_imp,         t_detector_hourly)\n",
    "        _merge_region_hourly(spark,   region_hourly,   t_region_hourly)\n",
    "        _merge_region_monthly(spark,  region_monthly,  t_region_monthly)\n",
    "        _merge_detector_congestion(spark, detector_congestion, t_detector_cong)\n",
    "\n",
    "        print(f\"Batch {batch_id}: merged {det_imp.count()} detector-hrs; \"\n",
    "              f\"{region_hourly.count()} region-hrs; {region_monthly.count()} region-months.\")\n",
    "\n",
    "    return foreach_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e5846a3-c97a-4313-9b53-b2fbc1f90055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------- orchestration ----------\n",
    "conf  = Config()\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "_bootstrap_uc(spark, conf.catalog, conf.db_name)\n",
    "\n",
    "silver_fact_fqn   = resolve_silver_fqn(conf)\n",
    "region_lookup_fqn = conf.table_fqn(conf.region_lookup)\n",
    "\n",
    "# targets\n",
    "t_region_hourly   = conf.table_fqn(\"traffic_gold_region_hourly\")\n",
    "t_detector_hourly = conf.table_fqn(\"traffic_gold_detector_hourly\")\n",
    "t_region_monthly  = conf.table_fqn(\"traffic_gold_region_monthly\")\n",
    "t_detector_cong   = conf.table_fqn(\"traffic_gold_detector_congestion\")\n",
    "t_gold_errors     = conf.table_fqn(\"traffic_gold_errors\")  # kept for parity (not used in MERGE path)\n",
    "\n",
    "ensure_region_lookup_coverage(spark, silver_fact_fqn, region_lookup_fqn, backfill_unknown=True)\n",
    "\n",
    "# precreate (first run) so MERGE targets exist\n",
    "precreate_gold_tables(\n",
    "    spark=spark,\n",
    "    silver_fact_fqn=silver_fact_fqn,\n",
    "    region_lookup_fqn=region_lookup_fqn,\n",
    "    t_region_hourly=t_region_hourly,\n",
    "    t_detector_hourly=t_detector_hourly,\n",
    "    t_region_monthly=t_region_monthly,\n",
    "    t_detector_cong=t_detector_cong,\n",
    "    dev_mode=DEV_MODE\n",
    ")\n",
    "\n",
    "# CDF streaming source from Silver (ignore deletes; use postimages)\n",
    "streaming_df = (\n",
    "    spark.readStream\n",
    "         .format(\"delta\")\n",
    "         .option(\"readChangeFeed\", \"true\")\n",
    "         .option(\"startingVersion\", CDF_FROM_VERSION)   # or .option(\"startingTimestamp\",\"2025-01-01\")\n",
    "         .table(silver_fact_fqn)\n",
    "         .where(col(\"_change_type\").isin(\"insert\",\"update_postimage\"))\n",
    ")\n",
    "\n",
    "checkpoint_dir = f\"{conf.checkpoint_base}/gold_cdf/{silver_fact_fqn.replace('.','_')}\"\n",
    "\n",
    "foreach_batch = make_foreach_batch_cdf_merge(\n",
    "    silver_fact_fqn=silver_fact_fqn,\n",
    "    region_lookup_fqn=region_lookup_fqn,\n",
    "    thresholds=THRESHOLDS,\n",
    "    t_region_hourly=t_region_hourly,\n",
    "    t_detector_hourly=t_detector_hourly,\n",
    "    t_region_monthly=t_region_monthly,\n",
    "    t_detector_cong=t_detector_cong\n",
    ")\n",
    "\n",
    "q = (streaming_df.writeStream\n",
    "     .foreachBatch(foreach_batch)\n",
    "     .option(\"checkpointLocation\", checkpoint_dir)\n",
    "     .outputMode(\"update\")   # foreachBatch ignores this for MERGE; harmless\n",
    "     .trigger(once=True)\n",
    "     .start())\n",
    "q.awaitTermination()\n",
    "\n",
    "# --- smoke/validate (optional) ---\n",
    "for t in [t_region_hourly, t_detector_hourly, t_region_monthly, t_detector_cong]:\n",
    "    try:\n",
    "        print(t, spark.table(t).count(), \"rows\")\n",
    "    except Exception as e:\n",
    "        print(\"Missing:\", t, e)\n",
    "print(\"✅ gold (CDF + MERGE) completed.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4702320059868562,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "5_gold_loader.py",
   "widgets": {
    "ENV": {
     "currentValue": "dev",
     "nuid": "87ba2e25-d59e-414c-8236-2d4a0f1e7d22",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "qa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev",
        "qa"
       ]
      }
     }
    },
    "METASTORE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "7ef4ff7d-6700-4b73-94bc-c700e9465063",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "STORAGE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "73c9b908-604e-48b5-b129-1b8811889172",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
