{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c1d0a6-7f97-47d7-b89a-e7c7d1b1889c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./1_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07444b15-dc87-41e1-8c41-ea3a7505a633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- inline params (dev/qa) ---\n",
    "import os, importlib\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"ENV\", \"dev\", [\"dev\", \"qa\"], \"Environment\")\n",
    "    dbutils.widgets.dropdown(\"STORAGE_ACCOUNT\", \"trafficsa2\", [\"trafficsa2\", \"trafficsaqa\"], \"Storage account\")\n",
    "    dbutils.widgets.text(\"METASTORE_ACCOUNT\", \"trafficsa2\", \"Metastore account\")\n",
    "    os.environ[\"ENV\"] = dbutils.widgets.get(\"ENV\").strip().lower()\n",
    "    os.environ[\"STORAGE_ACCOUNT\"] = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\n",
    "    os.environ[\"METASTORE_ACCOUNT\"] = (dbutils.widgets.get(\"METASTORE_ACCOUNT\") or os.environ[\"STORAGE_ACCOUNT\"]).strip()\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c5a438e-24db-4309-b879-2215fa14a6f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---- imports ----\n",
    "from typing import Optional, List\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, to_timestamp, from_unixtime, unix_timestamp, expr,\n",
    "    concat_ws, sha2, upper, trim, coalesce, current_timestamp\n",
    ")\n",
    "\n",
    "FIFTEEN_MIN = 15 * 60  # seconds\n",
    "\n",
    "class SilverLoader:\n",
    "    def __init__(self, conf: Optional[Config] = None):\n",
    "        self.conf = conf or Config()\n",
    "        self.spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "        self.catalog = self.conf.catalog\n",
    "        self.db = self.conf.db_name\n",
    "        self.bronze_fqn = self.conf.table_fqn(self.conf.bronze_table)\n",
    "        self.silver_table = self.conf.silver_table            # canonical name from Config\n",
    "        self.silver_fqn = self.conf.table_fqn(self.silver_table)\n",
    "        self.chk_dir = f\"{self.conf.checkpoint_base}/silver/{self.silver_table.replace('.','_')}\"\n",
    "\n",
    "    def _bootstrap_uc(self):\n",
    "        s = self.spark\n",
    "        s.sql(f\"CREATE CATALOG IF NOT EXISTS {self.catalog}\")\n",
    "        s.sql(f\"CREATE SCHEMA IF NOT EXISTS {self.catalog}.{self.db}\")\n",
    "        s.sql(f\"USE CATALOG {self.catalog}\")\n",
    "        s.sql(f\"USE {self.db}\")\n",
    "\n",
    "    def _create_silver_if_missing(self):\n",
    "        self.spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {self.silver_fqn} (\n",
    "            DetKey STRING,\n",
    "            Fact_ID STRING,\n",
    "            NM_REGION STRING,\n",
    "            NB_SCATS_SITE INT,\n",
    "            NB_DETECTOR INT,\n",
    "            Interval_EndTime TIMESTAMP,\n",
    "            Volume BIGINT,\n",
    "            load_time TIMESTAMP,\n",
    "            PartitionDate DATE\n",
    "        )\n",
    "        USING DELTA\n",
    "        TBLPROPERTIES (\n",
    "          delta.enableChangeDataFeed = true,\n",
    "          delta.constraints.nonneg_volume = 'Volume >= 0'\n",
    "        )\n",
    "        \"\"\")\n",
    "\n",
    "    # ---------- transforms ----------\n",
    "    @staticmethod\n",
    "    def _clean_negatives(df: DataFrame) -> DataFrame:\n",
    "        for i in range(96):\n",
    "            c = f\"V{i:02d}\"\n",
    "            if c in df.columns:\n",
    "                df = df.withColumn(c, when(col(c).cast(\"long\") < 0, lit(0)).otherwise(col(c).cast(\"long\")))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _unpivot_96(df: DataFrame) -> DataFrame:\n",
    "        labels = [f\"V{i:02d}\" for i in range(96)]\n",
    "        present = [c for c in labels if c in df.columns]\n",
    "        n = len(present)\n",
    "        stack_expr = \", \".join([f\"'{c}', {c}\" for c in present])\n",
    "        out = df.selectExpr(\n",
    "            \"NB_SCATS_SITE\",\n",
    "            \"QT_INTERVAL_COUNT\",\n",
    "            \"NB_DETECTOR\",\n",
    "            \"NM_REGION\",\n",
    "            f\"stack({n}, {stack_expr}) as (Interval_Label, Volume)\"\n",
    "        )\n",
    "        out = out.withColumn(\"Interval_Index\", expr(\"int(substring(Interval_Label, 2, 2))\"))\n",
    "        return out.drop(\"Interval_Label\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_interval_end(df: DataFrame) -> DataFrame:\n",
    "        base = df.withColumn(\"QT_TS\", to_timestamp(\"QT_INTERVAL_COUNT\"))\n",
    "        out = base.withColumn(\n",
    "            \"Interval_EndTime\",\n",
    "            from_unixtime(\n",
    "                unix_timestamp(\"QT_TS\") - ((95 - col(\"Interval_Index\")) * FIFTEEN_MIN)\n",
    "            ).cast(\"timestamp\")\n",
    "        )\n",
    "        return out.drop(\"QT_TS\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_and_key(df: DataFrame) -> DataFrame:\n",
    "        df = (df.withColumn(\"NM_REGION\", upper(trim(col(\"NM_REGION\"))))\n",
    "                .withColumn(\"Volume\", coalesce(col(\"Volume\").cast(\"long\"), lit(0))))\n",
    "        end_norm = F.from_unixtime((F.unix_timestamp(\"Interval_EndTime\")/FIFTEEN_MIN).cast(\"bigint\")*FIFTEEN_MIN).cast(\"timestamp\")\n",
    "        df = df.withColumn(\"Interval_EndTime\", end_norm)\n",
    "        detkey = sha2(concat_ws(\"§\",\"NM_REGION\",\"NB_SCATS_SITE\",\"NB_DETECTOR\", col(\"Interval_EndTime\").cast(\"string\")), 256)\n",
    "        df = (df.withColumn(\"DetKey\", detkey)\n",
    "                .withColumn(\"Fact_ID\", detkey)\n",
    "                .withColumn(\"load_time\", current_timestamp())\n",
    "                .withColumn(\"PartitionDate\", F.to_date(\"Interval_EndTime\")))\n",
    "        return df.dropDuplicates([\"DetKey\"])\n",
    "\n",
    "    def transform(self, bronze_batch: DataFrame) -> DataFrame:\n",
    "        df = bronze_batch\n",
    "        df = self._clean_negatives(df)\n",
    "        df = self._unpivot_96(df)\n",
    "        df = self._compute_interval_end(df)\n",
    "        df = self._normalize_and_key(df)\n",
    "        return df.select(\"DetKey\",\"Fact_ID\",\"NM_REGION\",\"NB_SCATS_SITE\",\"NB_DETECTOR\",\"Interval_EndTime\",\"Volume\",\"load_time\",\"PartitionDate\")\n",
    "\n",
    "    # ---------- MERGE ----------\n",
    "    def _merge_upsert(self, micro: DataFrame):\n",
    "        micro.createOrReplaceTempView(\"silver_updates\")\n",
    "        self.spark.sql(f\"\"\"\n",
    "        MERGE INTO {self.silver_fqn} AS t\n",
    "        USING silver_updates AS s\n",
    "        ON t.DetKey = s.DetKey\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "          t.Fact_ID = s.Fact_ID,\n",
    "          t.NM_REGION = s.NM_REGION,\n",
    "          t.NB_SCATS_SITE = s.NB_SCATS_SITE,\n",
    "          t.NB_DETECTOR = s.NB_DETECTOR,\n",
    "          t.Interval_EndTime = s.Interval_EndTime,\n",
    "          t.Volume = s.Volume,\n",
    "          t.load_time = s.load_time,\n",
    "          t.PartitionDate = s.PartitionDate\n",
    "        WHEN NOT MATCHED THEN INSERT (\n",
    "          DetKey, Fact_ID, NM_REGION, NB_SCATS_SITE, NB_DETECTOR, Interval_EndTime, Volume, load_time, PartitionDate\n",
    "        ) VALUES (\n",
    "          s.DetKey, s.Fact_ID, s.NM_REGION, s.NB_SCATS_SITE, s.NB_DETECTOR, s.Interval_EndTime, s.Volume, s.load_time, s.PartitionDate\n",
    "        )\n",
    "        \"\"\")\n",
    "\n",
    "    def make_foreach_batch(self):\n",
    "        def foreach_batch(batch_df: DataFrame, batch_id: int):\n",
    "            if batch_df.isEmpty():\n",
    "                print(f\"Batch {batch_id}: no data.\")\n",
    "                return\n",
    "            print(f\"Batch {batch_id}: transforming + MERGE upsert ...\")\n",
    "            self._merge_upsert(self.transform(batch_df))\n",
    "            print(f\"Batch {batch_id}: upsert complete.\")\n",
    "        return foreach_batch\n",
    "\n",
    "    def run_once(self):\n",
    "        self._bootstrap_uc()\n",
    "        self._create_silver_if_missing()\n",
    "        src = self.spark.readStream.format(\"delta\").table(self.bronze_fqn)\n",
    "        q = (src.writeStream\n",
    "                 .foreachBatch(self.make_foreach_batch())\n",
    "                 .option(\"checkpointLocation\", self.chk_dir)\n",
    "                 .outputMode(\"update\")\n",
    "                 .trigger(once=True)\n",
    "                 .start())\n",
    "        q.awaitTermination()\n",
    "\n",
    "    def validate(self):\n",
    "        df = self.spark.table(self.silver_fqn)\n",
    "        by_detkey = df.groupBy(\"DetKey\").count().filter(\"count > 1\").count()\n",
    "        assert by_detkey == 0, \"Silver contains duplicate DetKey rows.\"\n",
    "        kdup = (df.groupBy(\"NM_REGION\",\"NB_SCATS_SITE\",\"NB_DETECTOR\",\"Interval_EndTime\")\n",
    "                  .count().filter(\"count > 1\").count())\n",
    "        assert kdup == 0, \"Silver contains duplicate detector-hour rows.\"\n",
    "        print(f\"✅ {self.silver_fqn}: no duplicate keys.\")\n",
    "\n",
    "\n",
    "# run cell (silver)\n",
    "loader = SilverLoader()   # reads same ENV\n",
    "loader.run_once()         # trigger(once=True) + awaitTermination()\n",
    "loader.validate()         # raises if dup keys etc.\n",
    "print(\"✅ silver upsert/validate OK\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5106598202800195,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4_silver_loader.py",
   "widgets": {
    "ENV": {
     "currentValue": "dev",
     "nuid": "76aeb9aa-134f-4dce-a9d9-ea7ed5e00457",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "qa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "dev",
        "qa"
       ]
      }
     }
    },
    "METASTORE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "a326c1da-c711-4edf-af3e-e6dc7509d703",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "STORAGE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "e5b36c1f-86ba-45fa-b149-e136b3fa6043",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
