{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5926f8a1-9be9-4130-bfaa-d596c5579673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./1_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e85ce501-e10a-44f5-8d33-cc125bf61d37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- inline params (dev/qa) â€” optional; keep if running this as a notebook ---\n",
    "import os, importlib\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"ENV\", \"dev\", [\"dev\", \"qa\"], \"Environment\")\n",
    "    dbutils.widgets.dropdown(\"STORAGE_ACCOUNT\", \"trafficsa2\", [\"trafficsa2\", \"trafficsaqa\"], \"Storage account\")\n",
    "    dbutils.widgets.text(\"METASTORE_ACCOUNT\", \"trafficsa2\", \"Metastore account\")\n",
    "    os.environ[\"ENV\"] = dbutils.widgets.get(\"ENV\").strip().lower()\n",
    "    os.environ[\"STORAGE_ACCOUNT\"] = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\n",
    "    os.environ[\"METASTORE_ACCOUNT\"] = (dbutils.widgets.get(\"METASTORE_ACCOUNT\") or os.environ[\"STORAGE_ACCOUNT\"]).strip()\n",
    "except NameError:\n",
    "    # running as a pure module; ENV already set or defaults used\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "520d3d49-1254-4c26-b3b5-bc6e1efcd631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SetupHelper:\n",
    "    def __init__(self, conf_obj=None):\n",
    "        self.conf = conf_obj or conf\n",
    "        self.catalog  = self.conf.catalog\n",
    "        self.db_name  = self.conf.db_name\n",
    "        self.landing_zone   = self.conf.raw_data_path\n",
    "        self.checkpoint_base= self.conf.checkpoint_base\n",
    "        self.initialized = False\n",
    "\n",
    "    # ---------- catalog & schema ----------\n",
    "    def create_db(self):\n",
    "        spark.catalog.clearCache()\n",
    "        print(f\"Creating schema {self.catalog}.{self.db_name}...\", end='')\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS {self.catalog}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {self.catalog}.{self.db_name}\")\n",
    "        spark.sql(f\"USE CATALOG {self.catalog}\")\n",
    "        spark.sql(f\"USE {self.db_name}\")\n",
    "        self.initialized = True\n",
    "        print(\"Done\")\n",
    "\n",
    "    # ---------- Bronze landing ----------\n",
    "    def raw_traffic(self):\n",
    "        assert self.initialized, \"Database is not initialized.\"\n",
    "        print(\"Creating raw_traffic table...\", end='')\n",
    "        cols96 = \",\\n                \".join([f\"V{i:02d} INT\" for i in range(96)])\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.catalog}.{self.db_name}.raw_traffic (\n",
    "                NB_SCATS_SITE INT,\n",
    "                QT_INTERVAL_COUNT STRING,        -- date string of the reading day\n",
    "                NB_DETECTOR INT,\n",
    "                {cols96},                        -- 96 x 15-min volumes\n",
    "                NM_REGION STRING,\n",
    "                CT_RECORDS INT,\n",
    "                QT_VOLUME_24HOUR INT,\n",
    "                CT_ALARM_24HOUR INT,\n",
    "                PartitionDate DATE,\n",
    "                load_time TIMESTAMP,\n",
    "                source_file STRING\n",
    "            )\n",
    "            USING DELTA\n",
    "            PARTITIONED BY (PartitionDate)\n",
    "        \"\"\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    # ---------- Dimensions ----------\n",
    "    def dim_time(self):\n",
    "        assert self.initialized, \"Database is not initialized.\"\n",
    "        print(\"Creating dim_time table...\", end='')\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.catalog}.{self.db_name}.dim_time (\n",
    "                Date DATE,             -- calendar date\n",
    "                Hour INT,              -- 0-23\n",
    "                Year INT,\n",
    "                Month INT,             -- 1-12\n",
    "                DayOfWeek STRING,      -- Mon..Sun\n",
    "                WeekdayFlag BOOLEAN    -- True for Mon-Fri\n",
    "            )\n",
    "            USING DELTA\n",
    "        \"\"\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    def dim_detector(self):\n",
    "        \"\"\"\n",
    "        NOTE: CT_ALARM_24HOUR is included as requested.\n",
    "        Treat it as a 'snapshot' attribute (e.g., last loaded day) and\n",
    "        do not aggregate analysis off this column; use facts for that.\n",
    "        \"\"\"\n",
    "        assert self.initialized, \"Database is not initialized.\"\n",
    "        print(\"Creating dim_detector table...\", end='')\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.catalog}.{self.db_name}.dim_detector (\n",
    "                NB_DETECTOR INT,\n",
    "                NB_SCATS_SITE INT,\n",
    "                NM_REGION STRING,\n",
    "                SUBURB STRING\n",
    "            )\n",
    "            USING DELTA\n",
    "        \"\"\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    def region_lookup(self):\n",
    "        assert self.initialized, \"Database is not initialized.\"\n",
    "        print(\"Ensuring region_lookup table...\", end='')\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.catalog}.{self.db_name}.region_lookup (\n",
    "                NM_REGION STRING,\n",
    "                SUBURB   STRING\n",
    "            )\n",
    "            USING DELTA\n",
    "        \"\"\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    # ---------- (Empty) Gold facts: will be populated by your Gold loader ----------\n",
    "    def fact_traffic_15min(self):\n",
    "        \"\"\"\n",
    "        Fact at 15-minute grain. Populated by 5_gold_loader after reshaping V00..V95.\n",
    "        Partitioned by Year/Month for pruning.\n",
    "        \"\"\"\n",
    "        assert self.initialized, \"Database is not initialized.\"\n",
    "        print(\"Creating fact_traffic_15min table (empty schema)...\", end='')\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.catalog}.{self.db_name}.fact_traffic_15min (\n",
    "                TimeKey BIGINT,                 -- yyyymmddHHMM from IntervalStartTime\n",
    "                DetectorKey STRING,             -- hash or surrogate from (NB_SCATS_SITE, NB_DETECTOR)\n",
    "                SiteKey STRING,                 -- hash of NB_SCATS_SITE\n",
    "                RegionKey STRING,               -- hash of NM_REGION\n",
    "                IntervalStartTime TIMESTAMP,\n",
    "                Volume BIGINT,\n",
    "                Year INT,\n",
    "                Month INT\n",
    "            )\n",
    "            USING DELTA\n",
    "            PARTITIONED BY (Year, Month)\n",
    "        \"\"\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    def fact_daily_summary(self):\n",
    "        \"\"\"\n",
    "        Optional daily fact for quick daily cards/tiles.\n",
    "        Populated by 5_gold_loader from QT_VOLUME_24HOUR / CT_ALARM_24HOUR.\n",
    "        \"\"\"\n",
    "        assert self.initialized, \"Database is not initialized.\"\n",
    "        print(\"Creating fact_daily_summary table (empty schema)...\", end='')\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.catalog}.{self.db_name}.fact_daily_summary (\n",
    "                DateKey INT,                    -- yyyymmdd\n",
    "                DetectorKey STRING,\n",
    "                SiteKey STRING,\n",
    "                RegionKey STRING,\n",
    "                ReadingDate DATE,\n",
    "                TotalVolume BIGINT,             -- QT_VOLUME_24HOUR\n",
    "                AlarmCount BIGINT,              -- CT_ALARM_24HOUR\n",
    "                IntervalsWithData INT           -- CT_RECORDS\n",
    "            )\n",
    "            USING DELTA\n",
    "        \"\"\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    # ---------- Orchestration ----------\n",
    "    def setup(self):\n",
    "        import time\n",
    "        t0 = time.time()\n",
    "        print(\"\\nStarting setup ...\")\n",
    "        self.create_db()\n",
    "        self.raw_traffic()\n",
    "        self.dim_time()\n",
    "        self.dim_detector()     # now includes CT_ALARM_24HOUR\n",
    "        self.region_lookup()\n",
    "        # Pre-create empty fact tables so downstream Gold loader can overwrite/append safely\n",
    "        self.fact_traffic_15min()\n",
    "        self.fact_daily_summary()\n",
    "        print(f\"Setup completed in {int(time.time() - t0)} seconds\")\n",
    "\n",
    "    def assert_table(self, table):\n",
    "        result = (spark.sql(f\"SHOW TABLES IN {self.catalog}.{self.db_name}\")\n",
    "                    .filter(f\"tableName = '{table}'\"))\n",
    "        assert result.count() == 1, f\"The table '{table}' is missing\"\n",
    "        print(f\"Found {self.catalog}.{self.db_name}.{table}: Success\")\n",
    "\n",
    "    def validate(self):\n",
    "        import time\n",
    "        t0 = time.time()\n",
    "        print(\"\\nStarting setup validation ...\")\n",
    "        assert (spark.sql(f\"SHOW DATABASES IN {self.catalog}\")\n",
    "                  .filter(f\"databaseName == '{self.db_name}'\").count() == 1), \\\n",
    "               f\"The database '{self.catalog}.{self.db_name}' is missing\"\n",
    "        for tbl in [\n",
    "            \"raw_traffic\",\n",
    "            \"dim_time\",\n",
    "            \"dim_detector\",\n",
    "            \"region_lookup\",\n",
    "            \"fact_traffic_15min\",\n",
    "            \"fact_daily_summary\"\n",
    "        ]:\n",
    "            self.assert_table(tbl)\n",
    "        print(f\"Setup validation completed in {int(time.time() - t0)} seconds\")\n",
    "\n",
    "\n",
    "# Example interactive:\n",
    "setup = SetupHelper()\n",
    "setup.setup()\n",
    "setup.validate()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2_setup.py",
   "widgets": {
    "ENV": {
     "currentValue": "dev",
     "nuid": "25fe948a-0971-4427-a75e-03ce72532097",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "qa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev",
        "qa"
       ]
      }
     }
    },
    "METASTORE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "46458228-d524-448c-9a03-3dc264abeb01",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "STORAGE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "40dfb085-956e-4a9c-a0f9-83fa733fbb01",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
