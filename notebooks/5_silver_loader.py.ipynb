{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c1d0a6-7f97-47d7-b89a-e7c7d1b1889c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./1_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07444b15-dc87-41e1-8c41-ea3a7505a633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- inline params (dev/qa) ---\n",
    "import os, importlib\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"ENV\", \"dev\", [\"dev\", \"qa\"], \"Environment\")\n",
    "    dbutils.widgets.dropdown(\"STORAGE_ACCOUNT\", \"trafficsa2\", [\"trafficsa2\", \"trafficsaqa\"], \"Storage account\")\n",
    "    dbutils.widgets.text(\"METASTORE_ACCOUNT\", \"trafficsa2\", \"Metastore account\")\n",
    "    os.environ[\"ENV\"] = dbutils.widgets.get(\"ENV\").strip().lower()\n",
    "    os.environ[\"STORAGE_ACCOUNT\"] = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\n",
    "    os.environ[\"METASTORE_ACCOUNT\"] = (dbutils.widgets.get(\"METASTORE_ACCOUNT\") or os.environ[\"STORAGE_ACCOUNT\"]).strip()\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2368939-bccd-4bc9-9320-da99401bb8e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from typing import Optional, List\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Silver Loader\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "class SilverLoader:\n",
    "    \"\"\"\n",
    "    Silver layer for SCATS 'Traffic Signal Volume':\n",
    "      - Reads Bronze table: {catalog}.{db}.raw_traffic\n",
    "      - Light cleansing/standardization (region normalization, time parsing)\n",
    "      - Idempotent upsert via record hash MERGE\n",
    "      - Keeps the wide V00..V95 columns for Gold to reshape later\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conf_obj: Optional[\"Config\"] = None):\n",
    "        self.conf = conf_obj or conf\n",
    "        self.catalog = self.conf.catalog\n",
    "        self.db_name = self.conf.db_name\n",
    "        self.spark: SparkSession = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "        self.bronze_fqn = self.conf.table_fqn(self.conf.bronze_table)   # raw_traffic\n",
    "        self.silver_fqn = self.conf.table_fqn(self.conf.silver_table)   # e.g., traffic_silver\n",
    "\n",
    "        self._bootstrap_uc()\n",
    "        self._create_silver_if_not_exists()\n",
    "\n",
    "    # -------------------------- UC/bootstrap --------------------------\n",
    "    def _bootstrap_uc(self) -> None:\n",
    "        self.spark.sql(f\"CREATE CATALOG IF NOT EXISTS {self.catalog}\")\n",
    "        self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {self.catalog}.{self.db_name}\")\n",
    "        self.spark.sql(f\"USE CATALOG {self.catalog}\")\n",
    "        self.spark.sql(f\"USE {self.db_name}\")\n",
    "\n",
    "    # -------------------------- table DDL -----------------------------\n",
    "    def _create_silver_if_not_exists(self) -> None:\n",
    "        \"\"\"\n",
    "        Silver schema mirrors Bronze plus cleaned columns & keys:\n",
    "          - ReadingDate (DATE), ReadingTs (TIMESTAMP)\n",
    "          - NM_REGION normalized\n",
    "          - record_hash to support idempotent MERGE\n",
    "        \"\"\"\n",
    "        cols96 = \",\\n                \".join([f\"V{i:02d} INT\" for i in range(96)])\n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.silver_fqn} (\n",
    "                NB_SCATS_SITE INT,\n",
    "                NB_DETECTOR INT,\n",
    "                QT_INTERVAL_COUNT STRING,\n",
    "                {cols96},\n",
    "                NM_REGION STRING,\n",
    "                CT_RECORDS INT,\n",
    "                QT_VOLUME_24HOUR INT,\n",
    "                CT_ALARM_24HOUR INT,\n",
    "                PartitionDate DATE,\n",
    "                load_time TIMESTAMP,\n",
    "                source_file STRING,\n",
    "\n",
    "                -- normalized/derived\n",
    "                ReadingDate DATE,\n",
    "                ReadingTs TIMESTAMP,\n",
    "                NM_REGION_NORM STRING,\n",
    "\n",
    "                -- deterministic hash for idempotent upsert\n",
    "                record_hash STRING\n",
    "            )\n",
    "            USING DELTA\n",
    "            PARTITIONED BY (PartitionDate)\n",
    "        \"\"\")\n",
    "\n",
    "    # -------------------------- transforms ----------------------------\n",
    "    @staticmethod\n",
    "    def _parse_ts(df: DataFrame) -> DataFrame:\n",
    "        ts = F.coalesce(\n",
    "            F.to_timestamp(F.col(\"QT_INTERVAL_COUNT\"), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "            F.to_timestamp(F.col(\"QT_INTERVAL_COUNT\"), \"yyyy/MM/dd HH:mm:ss\"),\n",
    "            F.to_timestamp(F.col(\"QT_INTERVAL_COUNT\"), \"dd/MM/yyyy HH:mm:ss\"),\n",
    "            F.to_timestamp(F.col(\"QT_INTERVAL_COUNT\"), \"yyyy-MM-dd\"),\n",
    "            F.to_timestamp(F.col(\"QT_INTERVAL_COUNT\"), \"yyyy/MM/dd\"),\n",
    "            F.to_timestamp(F.col(\"QT_INTERVAL_COUNT\"))\n",
    "        )\n",
    "        return df.withColumn(\"ReadingTs\", ts).withColumn(\"ReadingDate\", F.to_date(ts))\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_region(df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn(\"NM_REGION_NORM\", F.upper(F.trim(F.col(\"NM_REGION\"))))\n",
    "\n",
    "    @staticmethod\n",
    "    def _record_hash(df: DataFrame) -> DataFrame:\n",
    "        # A stable hash across site, detector, reading date, source file.\n",
    "        # If you reload the same file, MERGE will deduplicate.\n",
    "        return df.withColumn(\n",
    "            \"record_hash\",\n",
    "            F.sha2(\n",
    "                F.concat_ws(\n",
    "                    \"||\",\n",
    "                    F.col(\"NB_SCATS_SITE\").cast(\"string\"),\n",
    "                    F.col(\"NB_DETECTOR\").cast(\"string\"),\n",
    "                    F.date_format(F.col(\"ReadingDate\"), \"yyyy-MM-dd\"),\n",
    "                    F.coalesce(F.col(\"source_file\"), F.lit(\"\"))\n",
    "                ),\n",
    "                256\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _select_and_cast(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Ensure numeric types for V00..V95 and metrics; preserve layout.\"\"\"\n",
    "        # Cast V00..V95\n",
    "        for i in range(96):\n",
    "            c = f\"V{i:02d}\"\n",
    "            if c in df.columns:\n",
    "                df = df.withColumn(c, F.col(c).cast(\"int\"))\n",
    "\n",
    "        # Cast tail metrics\n",
    "        cast_map = {\n",
    "            \"NB_SCATS_SITE\": \"int\",\n",
    "            \"NB_DETECTOR\": \"int\",\n",
    "            \"CT_RECORDS\": \"int\",\n",
    "            \"QT_VOLUME_24HOUR\": \"int\",\n",
    "            \"CT_ALARM_24HOUR\": \"int\"\n",
    "        }\n",
    "        for c, typ in cast_map.items():\n",
    "            if c in df.columns:\n",
    "                df = df.withColumn(c, F.col(c).cast(typ))\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _project_columns(self, df: DataFrame) -> DataFrame:\n",
    "        ordered = (\n",
    "            [\"NB_SCATS_SITE\", \"NB_DETECTOR\", \"QT_INTERVAL_COUNT\"]\n",
    "            + [f\"V{i:02d}\" for i in range(96)]\n",
    "            + [\"NM_REGION\", \"CT_RECORDS\", \"QT_VOLUME_24HOUR\", \"CT_ALARM_24HOUR\",\n",
    "               \"PartitionDate\", \"load_time\", \"source_file\",\n",
    "               \"ReadingDate\", \"ReadingTs\", \"NM_REGION_NORM\", \"record_hash\"]\n",
    "        )\n",
    "        existing = [c for c in ordered if c in df.columns]\n",
    "        return df.select(*existing)\n",
    "\n",
    "    # -------------------------- upsert logic --------------------------\n",
    "    def upsert_from_bronze(self, since_load_time: Optional[str] = None) -> int:\n",
    "        \"\"\"\n",
    "        Incrementally upserts from Bronze into Silver.\n",
    "        If since_load_time is provided (ISO timestamp string), only rows with load_time >= that are processed.\n",
    "        Returns number of rows written/updated.\n",
    "        \"\"\"\n",
    "        bronze = self.spark.table(self.bronze_fqn)\n",
    "\n",
    "        if since_load_time:\n",
    "            bronze = bronze.where(F.col(\"load_time\") >= F.to_timestamp(F.lit(since_load_time)))\n",
    "\n",
    "        # prepare\n",
    "        df = (bronze\n",
    "              .transform(self._select_and_cast)\n",
    "              .transform(self._parse_ts)\n",
    "              .transform(self._normalize_region)\n",
    "              .transform(self._record_hash)\n",
    "        )\n",
    "        df = self._project_columns(df).cache()\n",
    "\n",
    "        # MERGE keys\n",
    "        # Use record_hash as a single deterministic key\n",
    "        self.spark.sql(f\"CREATE TABLE IF NOT EXISTS {self.silver_fqn} USING DELTA PARTITIONED BY (PartitionDate) AS SELECT * FROM (SELECT * FROM {self.silver_fqn}) WHERE 1=0\")\n",
    "\n",
    "        df.createOrReplaceTempView(\"__incoming_silver\")\n",
    "\n",
    "        merge_sql = f\"\"\"\n",
    "        MERGE INTO {self.silver_fqn} AS tgt\n",
    "        USING __incoming_silver AS src\n",
    "          ON tgt.record_hash = src.record_hash\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        self.spark.sql(merge_sql)\n",
    "\n",
    "        # Count written\n",
    "        written = df.count()\n",
    "        df.unpersist()\n",
    "        return written\n",
    "\n",
    "    # -------------------------- utilities ----------------------------\n",
    "    def rebuild_all(self) -> int:\n",
    "        \"\"\"Full refresh: truncate and reload from Bronze.\"\"\"\n",
    "        self.spark.sql(f\"TRUNCATE TABLE {self.silver_fqn}\")\n",
    "        return self.upsert_from_bronze(since_load_time=None)\n",
    "\n",
    "    def validate(self, show_sample: int = 5) -> None:\n",
    "        df = self.spark.table(self.silver_fqn)\n",
    "        total = df.count()\n",
    "        parts = df.select(\"PartitionDate\").distinct().orderBy(\"PartitionDate\").collect()\n",
    "        first = parts[0][\"PartitionDate\"] if parts else None\n",
    "        last = parts[-1][\"PartitionDate\"] if parts else None\n",
    "        print(f\"‚úÖ {self.silver_fqn}: {total} rows across {len(parts)} partitions (first={first}, last={last}).\")\n",
    "        if show_sample:\n",
    "            display(df.limit(show_sample))\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Example usage (run as a notebook cell)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "SL = SilverLoader(conf)  # uses ENV from 1_config.py widgets if present\n",
    "rows = SL.rebuild_all()\n",
    "print(f\"üîÅ Silver rows written: {rows}\")\n",
    "SL.validate()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5106598202800195,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "5_silver_loader.py",
   "widgets": {
    "ENV": {
     "currentValue": "dev",
     "nuid": "76aeb9aa-134f-4dce-a9d9-ea7ed5e00457",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "qa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "dev",
        "qa"
       ]
      }
     }
    },
    "METASTORE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "a326c1da-c711-4edf-af3e-e6dc7509d703",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "STORAGE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "e5b36c1f-86ba-45fa-b149-e136b3fa6043",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
