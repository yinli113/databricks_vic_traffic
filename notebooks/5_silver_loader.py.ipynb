{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c1d0a6-7f97-47d7-b89a-e7c7d1b1889c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./1_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07444b15-dc87-41e1-8c41-ea3a7505a633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- inline params (dev/qa) ---\n",
    "import os, importlib\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"ENV\", \"dev\", [\"dev\", \"qa\"], \"Environment\")\n",
    "    dbutils.widgets.dropdown(\"STORAGE_ACCOUNT\", \"trafficsa2\", [\"trafficsa2\", \"trafficsaqa\"], \"Storage account\")\n",
    "    dbutils.widgets.text(\"METASTORE_ACCOUNT\", \"trafficsa2\", \"Metastore account\")\n",
    "    os.environ[\"ENV\"] = dbutils.widgets.get(\"ENV\").strip().lower()\n",
    "    os.environ[\"STORAGE_ACCOUNT\"] = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\n",
    "    os.environ[\"METASTORE_ACCOUNT\"] = (dbutils.widgets.get(\"METASTORE_ACCOUNT\") or os.environ[\"STORAGE_ACCOUNT\"]).strip()\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2368939-bccd-4bc9-9320-da99401bb8e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "class SilverLoader:\n",
    "    \"\"\"\n",
    "    Silver layer for SCATS 'Traffic Signal Volume':\n",
    "      - Reads Bronze table: {catalog}.{db}.raw_traffic\n",
    "      - Derives keys: TimeKey (15-min), DetectorKey, RegionKey\n",
    "      - Normalizes region_code\n",
    "      - Adds record_hash for idempotent merge\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conf_obj: Optional[\"Config\"] = None):\n",
    "        self.conf = conf_obj or conf\n",
    "        self.catalog = self.conf.catalog\n",
    "        self.db_name = self.conf.db_name\n",
    "        self.spark: SparkSession = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "        self.bronze_fqn = self.conf.table_fqn(self.conf.bronze_table)\n",
    "        self.silver_fqn = self.conf.table_fqn(self.conf.silver_table)\n",
    "\n",
    "        self._bootstrap_uc()\n",
    "        self._create_silver_if_not_exists()\n",
    "\n",
    "    # ---------------- bootstrap ----------------\n",
    "    def _bootstrap_uc(self):\n",
    "        self.spark.sql(f\"CREATE CATALOG IF NOT EXISTS {self.catalog}\")\n",
    "        self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {self.catalog}.{self.db_name}\")\n",
    "        self.spark.sql(f\"USE CATALOG {self.catalog}\")\n",
    "        self.spark.sql(f\"USE {self.db_name}\")\n",
    "\n",
    "    def _create_silver_if_not_exists(self):\n",
    "        cols96 = \",\\n                \".join([f\"V{i:02d} INT\" for i in range(96)])\n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.silver_fqn} (\n",
    "                TimeKey BIGINT,\n",
    "                DetectorKey BIGINT,\n",
    "                RegionKey BIGINT,\n",
    "                region_code STRING,\n",
    "                {cols96},\n",
    "                CT_RECORDS INT,\n",
    "                QT_VOLUME_24HOUR INT,\n",
    "                CT_ALARM_24HOUR INT,\n",
    "                PartitionDate DATE,\n",
    "                record_hash STRING\n",
    "            )\n",
    "            USING DELTA\n",
    "            PARTITIONED BY (PartitionDate)\n",
    "        \"\"\")\n",
    "\n",
    "    # ---------------- transforms ----------------\n",
    "    def _add_keys(self, df: DataFrame) -> DataFrame:\n",
    "        # parse timestamp\n",
    "        ts = F.coalesce(\n",
    "            F.to_timestamp(\"QT_INTERVAL_COUNT\", \"yyyy-MM-dd HH:mm:ss\"),\n",
    "            F.to_timestamp(\"QT_INTERVAL_COUNT\", \"yyyy/MM/dd HH:mm:ss\"),\n",
    "            F.to_timestamp(\"QT_INTERVAL_COUNT\", \"dd/MM/yyyy HH:mm:ss\"),\n",
    "            F.to_timestamp(\"QT_INTERVAL_COUNT\", \"yyyy-MM-dd\"),\n",
    "            F.to_timestamp(\"QT_INTERVAL_COUNT\", \"yyyy/MM/dd\"),\n",
    "            F.to_timestamp(\"QT_INTERVAL_COUNT\")\n",
    "        )\n",
    "        df = df.withColumn(\"ReadingTs\", ts)\n",
    "\n",
    "        # TimeKey = floor to 15 min (900 sec)\n",
    "        df = df.withColumn(\"TimeKey\", (F.unix_timestamp(\"ReadingTs\") / 900).cast(\"bigint\") * 900)\n",
    "\n",
    "        # DetectorKey = hash of site + detector\n",
    "        df = df.withColumn(\n",
    "            \"DetectorKey\",\n",
    "            F.crc32(F.concat_ws(\":\", \n",
    "                F.col(\"NB_SCATS_SITE\").cast(\"string\"),\n",
    "                F.col(\"NB_DETECTOR\").cast(\"string\"))\n",
    "            ).cast(\"bigint\")\n",
    "        )\n",
    "\n",
    "        # RegionKey from normalized region code\n",
    "        df = df.withColumn(\"region_code\",\n",
    "                           F.upper(F.trim(F.regexp_replace(\"NM_REGION\", r\"\\s+\", \"\"))))\n",
    "        df = df.withColumn(\"RegionKey\", F.crc32(F.col(\"region_code\")).cast(\"bigint\"))\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _add_hash(self, df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn(\n",
    "            \"record_hash\",\n",
    "            F.sha2(F.concat_ws(\"||\",\n",
    "                               F.coalesce(F.col(\"TimeKey\").cast(\"string\"), F.lit(\"\")),\n",
    "                               F.coalesce(F.col(\"DetectorKey\").cast(\"string\"), F.lit(\"\")),\n",
    "                               F.coalesce(F.col(\"RegionKey\").cast(\"string\"), F.lit(\"\"))), 256)\n",
    "        )\n",
    "\n",
    "    def _project(self, df: DataFrame) -> DataFrame:\n",
    "        base_cols = [\n",
    "            \"TimeKey\", \"DetectorKey\", \"RegionKey\",\n",
    "            \"NB_SCATS_SITE\", \"NB_DETECTOR\", \"QT_INTERVAL_COUNT\",\n",
    "            \"NM_REGION\", \"region_code\"\n",
    "        ]\n",
    "        vcols = [f\"V{i:02d}\" for i in range(96) if f\"V{i:02d}\" in df.columns]   # ‚úÖ keep all Vxx\n",
    "        other = [\n",
    "            \"CT_RECORDS\", \"QT_VOLUME_24HOUR\", \"CT_ALARM_24HOUR\",\n",
    "            \"PartitionDate\", \"record_hash\", \"ReadingDate\", \"ReadingTs\"\n",
    "        ]\n",
    "        cols = [c for c in base_cols + vcols + other if c in df.columns]\n",
    "        return df.select(*cols)\n",
    "\n",
    "    # ---------------- upsert logic ----------------\n",
    "    def upsert_from_bronze(self, since_load_time: Optional[str] = None) -> int:\n",
    "        try:\n",
    "            bronze = self.spark.table(self.bronze_fqn)\n",
    "        except AnalysisException:\n",
    "            raise RuntimeError(f\"‚ùå Bronze table {self.bronze_fqn} not found. Run Bronze loader first.\")\n",
    "\n",
    "        if since_load_time:\n",
    "            bronze = bronze.where(F.col(\"load_time\") >= F.to_timestamp(F.lit(since_load_time)))\n",
    "\n",
    "        df = (bronze\n",
    "              .transform(self._add_keys)\n",
    "              .transform(self._add_hash)\n",
    "              .transform(self._project)\n",
    "              .cache())\n",
    "\n",
    "        df.createOrReplaceTempView(\"__incoming_silver\")\n",
    "\n",
    "        cols = df.columns\n",
    "        update_set = \", \".join([f\"tgt.{c} = src.{c}\" for c in cols if c != \"record_hash\"])\n",
    "        insert_cols = \", \".join(cols)\n",
    "        insert_vals = \", \".join([f\"src.{c}\" for c in cols])\n",
    "\n",
    "        merge_sql = f\"\"\"\n",
    "        MERGE INTO {self.silver_fqn} AS tgt\n",
    "        USING __incoming_silver AS src\n",
    "        ON tgt.record_hash = src.record_hash\n",
    "        WHEN MATCHED THEN UPDATE SET {update_set}\n",
    "        WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals})\n",
    "        \"\"\"\n",
    "        self.spark.sql(merge_sql)\n",
    "\n",
    "        count = df.count()\n",
    "        df.unpersist()\n",
    "        return count\n",
    "\n",
    "    def rebuild_all(self) -> int:\n",
    "        self.spark.sql(f\"TRUNCATE TABLE {self.silver_fqn}\")\n",
    "        return self.upsert_from_bronze()\n",
    "\n",
    "    def validate(self, show_sample: int = 5):\n",
    "        df = self.spark.table(self.silver_fqn)\n",
    "        print(f\"‚úÖ {self.silver_fqn}: {df.count()} rows\")\n",
    "        if show_sample:\n",
    "            display(df.limit(show_sample))\n",
    "\n",
    "\n",
    "# ---------------- example ----------------\n",
    "SL = SilverLoader(conf)\n",
    "rows = SL.rebuild_all()\n",
    "print(f\"üîÅ Silver rows written: {rows}\")\n",
    "SL.validate()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5106598202800195,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "5_silver_loader.py",
   "widgets": {
    "ENV": {
     "currentValue": "dev",
     "nuid": "76aeb9aa-134f-4dce-a9d9-ea7ed5e00457",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "qa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "dev",
        "qa"
       ]
      }
     }
    },
    "METASTORE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "a326c1da-c711-4edf-af3e-e6dc7509d703",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "STORAGE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "e5b36c1f-86ba-45fa-b149-e136b3fa6043",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
