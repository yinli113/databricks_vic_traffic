{"cells": [{"cell_type": "code", "execution_count": 0, "metadata": {"application/vnd.databricks.v1+cell": {"cellMetadata": {"byteLimit": 2048000, "rowLimit": 10000}, "inputWidgets": {}, "nuid": "a0ffe0aa-7ef3-4ae8-baf7-84df83d022f5", "showTitle": false, "tableResultSettingsMap": {}, "title": ""}}, "outputs": [], "source": ["%run ./1_config.py"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"application/vnd.databricks.v1+cell": {"cellMetadata": {"byteLimit": 2048000, "rowLimit": 10000}, "inputWidgets": {}, "nuid": "81024517-c083-4230-81bc-aa0355e0fd42", "showTitle": false, "tableResultSettingsMap": {}, "title": ""}}, "outputs": [], "source": ["import os, importlib\n", "try:\n", "    dbutils.widgets.dropdown(\"ENV\", \"dev\", [\"dev\", \"qa\"], \"Environment\")\n", "    dbutils.widgets.dropdown(\"STORAGE_ACCOUNT\", \"trafficsa2\", [\"trafficsa2\", \"trafficsaqa\"], \"Storage account\")\n", "    dbutils.widgets.text(\"METASTORE_ACCOUNT\", \"trafficsa2\", \"Metastore account\")\n", "    os.environ[\"ENV\"] = dbutils.widgets.get(\"ENV\").strip().lower()\n", "    os.environ[\"STORAGE_ACCOUNT\"] = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\n", "    os.environ[\"METASTORE_ACCOUNT\"] = (dbutils.widgets.get(\"METASTORE_ACCOUNT\") or os.environ[\"STORAGE_ACCOUNT\"]).strip()\n", "except NameError:\n", "    pass\n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"application/vnd.databricks.v1+cell": {"cellMetadata": {}, "inputWidgets": {}, "nuid": "4af11ad3-a7de-4a50-ac01-7008061d358e", "showTitle": false, "tableResultSettingsMap": {}, "title": ""}}, "outputs": [], "source": ["from pyspark.sql import functions as F, SparkSession, DataFrame\n", "from pyspark.sql.utils import AnalysisException\n", "\n", "class GoldBuilder:\n", "    \"\"\"\n", "    Gold star schema built from Silver.\n", "      - Assumes Silver has: TimeKey (midnight, BIGINT), DetectorKey (BIGINT), RegionKey (BIGINT),\n", "        V00..V95, NB_SCATS_SITE, NB_DETECTOR, CT_RECORDS, QT_VOLUME_24HOUR, CT_ALARM_24HOUR.\n", "      - Melts V00..V95 to 15-min rows and computes INTERVAL TimeKey for dim_time + 15-min fact.\n", "      - Daily fact is derived from Silver's midnight TimeKey -> ReadingDate.\n", "      - dim_detector is built from Silver keys and enriched by dim_region (region_code/suburb).\n", "      - No SiteKey anywhere.\n", "    \"\"\"\n", "\n", "    def __init__(self, conf_obj: \"Config\" = None):\n", "        self.conf = conf_obj or conf\n", "        self.spark: SparkSession = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n", "\n", "        self.catalog = self.conf.catalog\n", "        self.db_name = self.conf.db_name\n", "\n", "        # FQNs\n", "        self.silver_fqn       = self.conf.table_fqn(self.conf.silver_table)\n", "        self.dim_time_fqn     = self.conf.table_fqn(\"dim_time\")\n", "        self.dim_detector_fqn = self.conf.table_fqn(\"dim_detector\")\n", "        self.dim_region_fqn   = self.conf.table_fqn(\"dim_region\")\n", "        self.fact_15_fqn      = self.conf.table_fqn(\"fact_traffic_15min\")\n", "        self.fact_daily_fqn   = self.conf.table_fqn(\"fact_daily_summary\")\n", "\n", "        self._bootstrap_uc()\n", "        self._ensure_dim_tables()\n", "        self._ensure_fact_tables()\n", "\n", "    # ---------------------------- bootstrap / DDL ----------------------------\n", "    def _bootstrap_uc(self):\n", "        self.spark.sql(f\"CREATE CATALOG IF NOT EXISTS {self.catalog}\")\n", "        self.spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS {self.catalog}.{self.db_name}\")\n", "        self.spark.sql(f\"USE CATALOG {self.catalog}\")\n", "        self.spark.sql(f\"USE {self.db_name}\")\n", "\n", "    def _ensure_dim_tables(self):\n", "        self.spark.sql(f\"\"\"\n", "            CREATE OR REPLACE TABLE {self.dim_time_fqn} (\n", "                TimeKey BIGINT,\n", "                Date DATE,\n", "                Hour INT,\n", "                Year INT,\n", "                Month INT,\n", "                DayOfWeek STRING,\n", "                WeekdayFlag BOOLEAN\n", "            ) USING DELTA\n", "        \"\"\")\n", "        self.spark.sql(f\"\"\"\n", "            CREATE OR REPLACE TABLE {self.dim_detector_fqn} (\n", "                DetectorKey BIGINT,\n", "                NB_SCATS_SITE INT,\n", "                NB_DETECTOR INT,\n", "                NM_REGION STRING,\n", "                RegionKey BIGINT,\n", "                suburb STRING\n", "            ) USING DELTA\n", "        \"\"\")\n", "\n", "    def _ensure_fact_tables(self):\n", "        self.spark.sql(f\"\"\"\n", "            CREATE OR REPLACE TABLE {self.fact_15_fqn} (\n", "                TimeKey BIGINT,\n", "                DetectorKey BIGINT,\n", "                RegionKey BIGINT,\n", "                IntervalStartTime TIMESTAMP,\n", "                Volume BIGINT,\n", "                Year INT,\n", "                Month INT\n", "            ) USING DELTA\n", "            PARTITIONED BY (Year, Month)\n", "        \"\"\")\n", "        self.spark.sql(f\"\"\"\n", "            CREATE OR REPLACE TABLE {self.fact_daily_fqn} (\n", "                DateKey INT,\n", "                DetectorKey BIGINT,\n", "                RegionKey BIGINT,\n", "                ReadingDate DATE,\n", "                TotalVolume BIGINT,\n", "                AlarmCount BIGINT,\n", "                IntervalsWithData INT\n", "            ) USING DELTA\n", "        \"\"\")\n", "\n", "    # ---------------------------- helpers ----------------------------\n", "    def _available_vcols(self, df: DataFrame):\n", "        return [f\"V{i:02d}\" for i in range(96) if f\"V{i:02d}\" in df.columns]\n", "\n", "    # Wide (V00..V95) -> long with IntervalStartTime and interval TimeKey (computed later)\n", "    def _melt_15min(self, df: DataFrame) -> DataFrame:\n", "        vcols = self._available_vcols(df)\n", "        if not vcols:\n", "            raise ValueError(\"No V00..V95 columns found in Silver. Rebuild Silver so it includes the volume columns.\")\n", "\n", "        base_cols = [\"DetectorKey\", \"NB_SCATS_SITE\", \"NB_DETECTOR\", \"RegionKey\", \"TimeKey\"]\n", "        sel_cols = [c for c in base_cols if c in df.columns] + vcols\n", "\n", "        df2 = (df.select(*sel_cols)\n", "                 .withColumn(\"volumes\", F.array(*[F.col(c).cast(\"long\") for c in vcols])))\n", "\n", "        exploded = df2.select(\n", "            *[c for c in base_cols if c in df2.columns],\n", "            F.posexplode_outer(\"volumes\").alias(\"IntervalIndex\", \"Volume\")\n", "        )\n", "\n", "        # Silver.TimeKey is midnight for the day (epoch seconds). Build interval start timestamps.\n", "        exploded = (exploded\n", "            .withColumn(\"MidnightTs\", F.to_timestamp(F.from_unixtime(F.col(\"TimeKey\"))))\n", "            .withColumn(\"IntervalStartTime\", F.expr(\"MidnightTs + make_interval(0,0,0,0,0, IntervalIndex*15, 0)\"))\n", "            .drop(\"MidnightTs\"))\n", "\n", "        return exploded\n", "\n", "    # ---------------------------- dimensions ----------------------------\n", "    # Use INTERVAL time (not Silver midnight) for dim_time PK\n", "    def _upsert_dim_time(self, intervals_df: DataFrame):\n", "        dim = (intervals_df\n", "               .withColumn(\"IntervalTimeKey\", F.col(\"IntervalStartTime\").cast(\"long\").cast(\"bigint\"))\n", "               .select(\n", "                   F.col(\"IntervalTimeKey\").alias(\"TimeKey\"),\n", "                   F.to_date(\"IntervalStartTime\").alias(\"Date\"),\n", "                   F.hour(\"IntervalStartTime\").alias(\"Hour\"),\n", "                   F.year(\"IntervalStartTime\").alias(\"Year\"),\n", "                   F.month(\"IntervalStartTime\").alias(\"Month\"),\n", "                   F.date_format(\"IntervalStartTime\", \"E\").alias(\"DayOfWeek\"),\n", "                   F.when(F.dayofweek(\"IntervalStartTime\").isin(1, 7), F.lit(False))\n", "                    .otherwise(F.lit(True)).alias(\"WeekdayFlag\")\n", "               )\n", "               .dropDuplicates([\"TimeKey\"])\n", "        )\n", "        dim.createOrReplaceTempView(\"__dim_time_src\")\n", "\n", "        self.spark.sql(f\"\"\"\n", "            MERGE INTO {self.dim_time_fqn} AS tgt\n", "            USING __dim_time_src AS src\n", "              ON tgt.TimeKey = src.TimeKey\n", "            WHEN MATCHED THEN UPDATE SET\n", "              tgt.Date = src.Date,\n", "              tgt.Hour = src.Hour,\n", "              tgt.Year = src.Year,\n", "              tgt.Month = src.Month,\n", "              tgt.DayOfWeek = src.DayOfWeek,\n", "              tgt.WeekdayFlag = src.WeekdayFlag\n", "            WHEN NOT MATCHED THEN INSERT (\n", "              TimeKey, Date, Hour, Year, Month, DayOfWeek, WeekdayFlag\n", "            ) VALUES (\n", "              src.TimeKey, src.Date, src.Hour, src.Year, src.Month, src.DayOfWeek, src.WeekdayFlag\n", "            )\n", "        \"\"\")\n", "\n", "    # Build from Silver keys, enrich from dim_region; avoid NM_REGION dependency in Silver\n", "    def _upsert_dim_detector(self, silver_df: DataFrame):\n", "        dim = (silver_df\n", "               .select(\"DetectorKey\", \"NB_SCATS_SITE\", \"NB_DETECTOR\", \"RegionKey\")\n", "               .dropna(subset=[\"DetectorKey\", \"NB_SCATS_SITE\", \"NB_DETECTOR\"])\n", "               .dropDuplicates())\n", "\n", "        try:\n", "            region = self.spark.table(self.dim_region_fqn).select(\"RegionKey\", \"region_code\", \"suburb\")\n", "            dim = dim.join(region, \"RegionKey\", \"left\")\n", "        except AnalysisException:\n", "            dim = dim.withColumn(\"region_code\", F.lit(None).cast(\"string\")) \\\n", "                     .withColumn(\"suburb\", F.lit(None).cast(\"string\"))\n", "\n", "        # Keep NM_REGION column for compatibility, populated from region_code\n", "        dim = dim.withColumn(\"NM_REGION\", F.col(\"region_code\")).drop(\"region_code\")\n", "\n", "        dim.createOrReplaceTempView(\"__dim_detector_src\")\n", "\n", "        self.spark.sql(f\"\"\"\n", "            MERGE INTO {self.dim_detector_fqn} AS tgt\n", "            USING __dim_detector_src AS src\n", "              ON tgt.DetectorKey = src.DetectorKey\n", "            WHEN MATCHED THEN UPDATE SET\n", "              tgt.NB_SCATS_SITE = src.NB_SCATS_SITE,\n", "              tgt.NB_DETECTOR   = src.NB_DETECTOR,\n", "              tgt.NM_REGION     = src.NM_REGION,\n", "              tgt.RegionKey     = src.RegionKey,\n", "              tgt.suburb        = src.suburb\n", "            WHEN NOT MATCHED THEN INSERT (\n", "              DetectorKey, NB_SCATS_SITE, NB_DETECTOR, NM_REGION, RegionKey, suburb\n", "            ) VALUES (\n", "              src.DetectorKey, src.NB_SCATS_SITE, src.NB_DETECTOR, src.NM_REGION, src.RegionKey, src.suburb\n", "            )\n", "        \"\"\")\n", "\n", "    # ---------------------------- facts ----------------------------\n", "    def _build_fact_15min(self, silver_df: DataFrame):\n", "        melted = self._melt_15min(silver_df).cache()\n", "\n", "        fact = (melted\n", "                .withColumn(\"TimeKey\", F.col(\"IntervalStartTime\").cast(\"long\").cast(\"bigint\"))\n", "                .withColumn(\"Year\", F.year(\"IntervalStartTime\"))\n", "                .withColumn(\"Month\", F.month(\"IntervalStartTime\"))\n", "                .select(\"TimeKey\", \"DetectorKey\", \"RegionKey\", \"IntervalStartTime\", \"Volume\", \"Year\", \"Month\"))\n", "\n", "        self.spark.sql(f\"TRUNCATE TABLE {self.fact_15_fqn}\")\n", "        (fact.write\n", "             .mode(\"append\")\n", "             .format(\"delta\")\n", "             .partitionBy(\"Year\", \"Month\")\n", "             .saveAsTable(self.fact_15_fqn))\n", "\n", "        rows = fact.count()\n", "        melted.unpersist()\n", "        return rows\n", "\n", "    def _build_fact_daily(self, silver_df: DataFrame):\n", "        # Silver.TimeKey is midnight; derive day directly from it\n", "        fact = (silver_df\n", "                .withColumn(\"ReadingDate\", F.to_date(F.from_unixtime(F.col(\"TimeKey\"))))\n", "                .select(\n", "                    F.date_format(\"ReadingDate\", \"yyyyMMdd\").cast(\"int\").alias(\"DateKey\"),\n", "                    \"DetectorKey\",\n", "                    \"RegionKey\",\n", "                    \"ReadingDate\",\n", "                    F.col(\"QT_VOLUME_24HOUR\").cast(\"long\").alias(\"TotalVolume\"),\n", "                    F.col(\"CT_ALARM_24HOUR\").cast(\"long\").alias(\"AlarmCount\"),\n", "                    F.col(\"CT_RECORDS\").cast(\"int\").alias(\"IntervalsWithData\")\n", "                ))\n", "\n", "        self.spark.sql(f\"TRUNCATE TABLE {self.fact_daily_fqn}\")\n", "        fact.write.mode(\"append\").format(\"delta\").saveAsTable(self.fact_daily_fqn)\n", "        return fact.count()\n", "\n", "    # ---------------------------- public APIs ----------------------------\n", "    def rebuild_all(self):\n", "        # Source\n", "        silver = self.spark.table(self.silver_fqn)\n", "\n", "        # Build dims\n", "        melted = self._melt_15min(silver)\n", "        self._upsert_dim_time(melted)\n", "        self._upsert_dim_detector(silver)\n", "\n", "        # Build facts\n", "        rows15 = self._build_fact_15min(silver)\n", "        rowsD  = self._build_fact_daily(silver)\n", "\n", "        print(f\"\u2705 Full Gold rebuild complete: 15min={rows15}, daily={rowsD}\")\n", "\n", "    def validate(self):\n", "        f15 = self.spark.table(self.fact_15_fqn)\n", "        fd  = self.spark.table(self.fact_daily_fqn)\n", "        print(f\"\ud83d\udd0e {self.fact_15_fqn}: {f15.count()} rows\")\n", "        try:\n", "            parts = self.spark.sql(f\"SHOW PARTITIONS {self.fact_15_fqn}\").collect()\n", "            if parts:\n", "                sample = \", \".join([r['partition'] for r in parts[:5]] + ([\"...\"] if len(parts) > 5 else []))\n", "                print(\"   partitions example:\", sample)\n", "        except Exception:\n", "            pass\n", "        print(f\"\ud83d\udd0e {self.fact_daily_fqn}: {fd.count()} rows\")\n", "\n", "\n", "# ---------------------------- example run ----------------------------\n", "GB = GoldBuilder(conf)\n", "GB.rebuild_all()\n", "GB.validate()"]}], "metadata": {"application/vnd.databricks.v1+notebook": {"computePreferences": null, "dashboards": [], "environmentMetadata": {"base_environment": "", "environment_version": "2"}, "inputWidgetPreferences": null, "language": "python", "notebookMetadata": {"mostRecentlyExecutedCommandWithImplicitDF": {"commandId": 4702320059868562, "dataframes": ["_sqldf"]}, "pythonIndentUnit": 4}, "notebookName": "6_gold_loader.py", "widgets": {"ENV": {"currentValue": "dev", "nuid": "87ba2e25-d59e-414c-8236-2d4a0f1e7d22", "typedWidgetInfo": {"autoCreated": false, "defaultValue": "dev", "label": "Environment", "name": "ENV", "options": {"widgetDisplayType": "Dropdown", "choices": ["dev", "qa"], "fixedDomain": true, "multiselect": false}, "parameterDataType": "String"}, "widgetInfo": {"widgetType": "dropdown", "defaultValue": "dev", "label": "Environment", "name": "ENV", "options": {"widgetType": "dropdown", "autoCreated": null, "choices": ["dev", "qa"]}}}, "METASTORE_ACCOUNT": {"currentValue": "trafficsa2", "nuid": "7ef4ff7d-6700-4b73-94bc-c700e9465063", "typedWidgetInfo": {"autoCreated": false, "defaultValue": "trafficsa2", "label": "Metastore account", "name": "METASTORE_ACCOUNT", "options": {"widgetDisplayType": "Text", "validationRegex": null}, "parameterDataType": "String"}, "widgetInfo": {"widgetType": "text", "defaultValue": "trafficsa2", "label": "Metastore account", "name": "METASTORE_ACCOUNT", "options": {"widgetType": "text", "autoCreated": null, "validationRegex": null}}}, "STORAGE_ACCOUNT": {"currentValue": "trafficsa2", "nuid": "73c9b908-604e-48b5-b129-1b8811889172", "typedWidgetInfo": {"autoCreated": false, "defaultValue": "trafficsa2", "label": "Storage account", "name": "STORAGE_ACCOUNT", "options": {"widgetDisplayType": "Dropdown", "choices": ["trafficsa2", "trafficsaqa"], "fixedDomain": true, "multiselect": false}, "parameterDataType": "String"}, "widgetInfo": {"widgetType": "dropdown", "defaultValue": "trafficsa2", "label": "Storage account", "name": "STORAGE_ACCOUNT", "options": {"widgetType": "dropdown", "autoCreated": null, "choices": ["trafficsa2", "trafficsaqa"]}}}}}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 0}
