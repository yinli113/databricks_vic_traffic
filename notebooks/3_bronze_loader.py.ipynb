{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8689485b-cece-46db-946e-a785760f4430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./1_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72cc9487-53ad-4fa0-ad4c-a011e9048310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- inline params (dev/qa) ---\n",
    "import os, importlib\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"ENV\", \"dev\", [\"dev\", \"qa\"], \"Environment\")\n",
    "    dbutils.widgets.dropdown(\"STORAGE_ACCOUNT\", \"trafficsa2\", [\"trafficsa2\", \"trafficsaqa\"], \"Storage account\")\n",
    "    dbutils.widgets.text(\"METASTORE_ACCOUNT\", \"trafficsa2\", \"Metastore account\")\n",
    "    os.environ[\"ENV\"] = dbutils.widgets.get(\"ENV\").strip().lower()\n",
    "    os.environ[\"STORAGE_ACCOUNT\"] = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\n",
    "    os.environ[\"METASTORE_ACCOUNT\"] = (dbutils.widgets.get(\"METASTORE_ACCOUNT\") or os.environ[\"STORAGE_ACCOUNT\"]).strip()\n",
    "except NameError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc475fd-3b4d-4620-984f-3e8b8bfcd202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Optional, List\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_timestamp, to_date, input_file_name,\n",
    "    to_timestamp, coalesce\n",
    ")\n",
    "\n",
    "class LoadRawTraffic:\n",
    "    \"\"\"\n",
    "    Bronze loader for SCATS 'Traffic Signal Volume' CSVs.\n",
    "    \"\"\"\n",
    "    def __init__(self, catalog: str, table_name: str, checkpoint_dir: Optional[str] = None, env: Optional[str] = None):\n",
    "        self.conf = Config(env or catalog)\n",
    "        self.catalog = catalog\n",
    "        self.db_name = self.conf.db_name\n",
    "        self.landing_zone = self.conf.raw_data_path\n",
    "        self.table_name = table_name\n",
    "        self.table_fqn = self.conf.table_fqn(table_name)\n",
    "        base_chk = f\"{self.conf.checkpoint_base}/bronze\"\n",
    "        self.checkpoint_dir = checkpoint_dir or f\"{base_chk}/{self.table_name.replace('.', '_')}\"\n",
    "        self.spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "    def _bootstrap_uc(self) -> None:\n",
    "        self.spark.sql(f\"CREATE CATALOG IF NOT EXISTS {self.catalog}\")\n",
    "        self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {self.catalog}.{self.db_name}\")\n",
    "        self.spark.sql(f\"USE CATALOG {self.catalog}\")\n",
    "        self.spark.sql(f\"USE {self.db_name}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _raw_table_columns_sql() -> str:\n",
    "        cols = [f\"V{i:02d} INT\" for i in range(96)]\n",
    "        return \",\\n                \".join(cols)\n",
    "\n",
    "    def _create_raw_table_if_not_exists(self) -> None:\n",
    "        cols96 = self._raw_table_columns_sql()\n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.table_fqn} (\n",
    "                NB_SCATS_SITE INT,\n",
    "                QT_INTERVAL_COUNT STRING,\n",
    "                NB_DETECTOR INT,\n",
    "                {cols96},\n",
    "                NM_REGION STRING,\n",
    "                CT_RECORDS INT,\n",
    "                QT_VOLUME_24HOUR INT,\n",
    "                CT_ALARM_24HOUR INT,\n",
    "                PartitionDate DATE,\n",
    "                load_time TIMESTAMP,\n",
    "                source_file STRING\n",
    "            )\n",
    "            USING DELTA\n",
    "            PARTITIONED BY (PartitionDate)\n",
    "        \"\"\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_partition_date(df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn(\n",
    "            \"PartitionDate\",\n",
    "            to_date(\n",
    "                coalesce(\n",
    "                    to_timestamp(col(\"QT_INTERVAL_COUNT\"), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "                    to_timestamp(col(\"QT_INTERVAL_COUNT\"), \"yyyy/MM/dd HH:mm:ss\"),\n",
    "                    to_timestamp(col(\"QT_INTERVAL_COUNT\"), \"dd/MM/yyyy HH:mm:ss\"),\n",
    "                    to_timestamp(col(\"QT_INTERVAL_COUNT\")),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _drop_if_exists(df: DataFrame, cols: List[str]) -> DataFrame:\n",
    "        for c in cols:\n",
    "            if c in df.columns:\n",
    "                df = df.drop(c)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _cast_volume_cols_int(df: DataFrame) -> DataFrame:\n",
    "        for i in range(96):\n",
    "            c = f\"V{i:02d}\"\n",
    "            if c in df.columns:\n",
    "                df = df.withColumn(c, col(c).cast(\"int\"))\n",
    "        return df\n",
    "\n",
    "    def create_db(self) -> None:\n",
    "        self._bootstrap_uc()\n",
    "        self._create_raw_table_if_not_exists()\n",
    "\n",
    "    def batch_load(self, start_date: str = \"2025-05-01\", end_date: str = \"2025-05-01\") -> None:\n",
    "        print(f\"üì¶ Batch load: {start_date} ‚Üí {end_date} from {self.landing_zone}\")\n",
    "        start = datetime.date.fromisoformat(start_date)\n",
    "        end = datetime.date.fromisoformat(end_date)\n",
    "\n",
    "        for day in (start + datetime.timedelta(n) for n in range((end - start).days + 1)):\n",
    "            filename = f\"VSDATA_{day.strftime('%Y%m%d')}.csv\"\n",
    "            path = f\"{self.landing_zone}/{filename}\"\n",
    "            try:\n",
    "                df = (self.spark.read.option(\"header\", True).csv(path))\n",
    "                if \"QT_INTERVAL_COUNT\" not in df.columns:\n",
    "                    raise ValueError(\"Column QT_INTERVAL_COUNT is missing in the file.\")\n",
    "\n",
    "                df = self._drop_if_exists(df, [\"_rescued_data\"])\n",
    "                df = self._cast_volume_cols_int(df)\n",
    "                df = (df.transform(self._parse_partition_date)\n",
    "                        .withColumn(\"load_time\", current_timestamp())\n",
    "                        .withColumn(\"source_file\", input_file_name()))\n",
    "\n",
    "                (df.write.format(\"delta\")\n",
    "                    .mode(\"append\")\n",
    "                    .option(\"mergeSchema\", \"true\")\n",
    "                    .partitionBy(\"PartitionDate\")\n",
    "                    .saveAsTable(self.table_fqn))\n",
    "\n",
    "                print(f\"‚úÖ Loaded {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to load {filename}: {e}\")\n",
    "\n",
    "    def stream_load(self, file_pattern: str = \"VSDATA_202506*.csv\", trigger_once: bool = True, reset_checkpoint: bool = True) -> None:\n",
    "        print(f\"üåä Streaming load for pattern {file_pattern}\")\n",
    "        stream_path = f\"{self.landing_zone}/{file_pattern}\"\n",
    "        stream_chk = f\"{self.checkpoint_dir}/streaming\"\n",
    "        schema_loc = f\"{stream_chk}/schema\"\n",
    "\n",
    "        if reset_checkpoint:\n",
    "            print(f\"üßπ Cleaning checkpoint: {stream_chk}\")\n",
    "            try:\n",
    "                dbutils.fs.rm(stream_chk, recurse=True)\n",
    "                print(\"‚úÖ Checkpoint cleared.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not clear checkpoint ({e}). Continuing...\")\n",
    "\n",
    "        reader = (self.spark.readStream.format(\"cloudFiles\")\n",
    "                    .option(\"cloudFiles.format\", \"csv\")\n",
    "                    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                    .option(\"header\", True)\n",
    "                    .option(\"cloudFiles.schemaLocation\", schema_loc)\n",
    "                    .load(stream_path))\n",
    "\n",
    "        stream_df = (reader.transform(lambda d: self._drop_if_exists(d, [\"_rescued_data\"]))\n",
    "                           .transform(self._cast_volume_cols_int)\n",
    "                           .transform(self._parse_partition_date)\n",
    "                           .withColumn(\"load_time\", current_timestamp())\n",
    "                           .withColumn(\"source_file\", col(\"_metadata.file_path\")))\n",
    "\n",
    "        writer = (stream_df.writeStream.format(\"delta\")\n",
    "                    .option(\"checkpointLocation\", stream_chk)\n",
    "                    .option(\"mergeSchema\", \"true\")\n",
    "                    .partitionBy(\"PartitionDate\")\n",
    "                    .outputMode(\"append\"))\n",
    "\n",
    "        writer = writer.trigger(once=True) if trigger_once else writer.trigger(availableNow=True)\n",
    "        query = writer.toTable(self.table_fqn)\n",
    "        query.awaitTermination()\n",
    "        print(\"‚úÖ Streaming load completed.\")\n",
    "\n",
    "    def validate_table(self) -> None:\n",
    "        print(f\"üîé Validating {self.table_fqn} ...\")\n",
    "        try:\n",
    "            df = self.spark.table(self.table_fqn)\n",
    "            total = df.count()\n",
    "            parts = df.select(\"PartitionDate\").distinct().orderBy(\"PartitionDate\").collect()\n",
    "            print(f\"‚úÖ {self.table_fqn}: {total} rows across {len(parts)} partitions.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Validation failed: {e}\")\n",
    "\n",
    "# run cell (bronze)\n",
    "conf = Config()  # uses your ENV/widgets from the top\n",
    "bronze = LoadRawTraffic(catalog=conf.catalog, table_name=conf.bronze_table, env=conf.env)\n",
    "\n",
    "bronze.create_db()\n",
    "bronze.batch_load(start_date=\"2025-05-01\", end_date=\"2025-05-01\")\n",
    "bronze.stream_load(file_pattern=\"VSDATA_202506*.csv\", trigger_once=True)  \n",
    "bronze.validate_table()  # raises on failure\n",
    "print(\"‚úÖ bronze load/validate OK\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3_bronze_loader.py",
   "widgets": {
    "ENV": {
     "currentValue": "dev",
     "nuid": "4c57c176-964c-4995-b3c0-b80430633ef0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "qa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "dev",
        "qa"
       ]
      }
     }
    },
    "METASTORE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "5dd4dfed-06c8-4aaa-a4a9-9e8567e7adcf",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "STORAGE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "512f814c-4e17-4be4-8de1-aa1f5af31b31",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
