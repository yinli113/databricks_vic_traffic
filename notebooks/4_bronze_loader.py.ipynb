{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8689485b-cece-46db-946e-a785760f4430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./1_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72cc9487-53ad-4fa0-ad4c-a011e9048310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- inline params (dev/qa) ---\n",
    "import os, importlib\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"ENV\", \"dev\", [\"dev\", \"qa\"], \"Environment\")\n",
    "    dbutils.widgets.dropdown(\"STORAGE_ACCOUNT\", \"trafficsa2\", [\"trafficsa2\", \"trafficsaqa\"], \"Storage account\")\n",
    "    dbutils.widgets.text(\"METASTORE_ACCOUNT\", \"trafficsa2\", \"Metastore account\")\n",
    "    os.environ[\"ENV\"] = dbutils.widgets.get(\"ENV\").strip().lower()\n",
    "    os.environ[\"STORAGE_ACCOUNT\"] = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\n",
    "    os.environ[\"METASTORE_ACCOUNT\"] = (dbutils.widgets.get(\"METASTORE_ACCOUNT\") or os.environ[\"STORAGE_ACCOUNT\"]).strip()\n",
    "except NameError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f6a4798-d586-48b2-a331-92d051dfbdb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional, List\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_timestamp, to_date, input_file_name,\n",
    "    to_timestamp, coalesce\n",
    ")\n",
    "\n",
    "# If Config is in 1_config.py, ensure it's imported in your notebook before running this file.\n",
    "# e.g., add at the top of the notebook:  %run ./1_config.py\n",
    "\n",
    "class LoadRawTraffic:\n",
    "    \"\"\"\n",
    "    Bronze loader for SCATS 'Traffic Signal Volume' CSVs.\n",
    "    Writes to {catalog}.{db}.raw_traffic (schema created here if not exists).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, catalog: str, table_name: str, checkpoint_dir: Optional[str] = None, env: Optional[str] = None):\n",
    "        self.conf = Config(env or os.getenv(\"ENV\") or \"dev\")   # keep your original behavior\n",
    "        self.catalog = catalog\n",
    "        self.db_name = self.conf.db_name\n",
    "        self.landing_zone = getattr(self.conf, \"raw_data_path\", getattr(self.conf, \"landing_zone\", None))\n",
    "        if not self.landing_zone:\n",
    "            raise ValueError(\"Config must expose raw_data_path or landing_zone for Bronze ingestion.\")\n",
    "        self.table_name = table_name\n",
    "        # prefer your helper if present, else compose FQN\n",
    "        self.table_fqn = getattr(self.conf, \"table_fqn\", lambda t: f\"{self.catalog}.{self.db_name}.{t}\")(table_name)\n",
    "        base_chk = f\"{getattr(self.conf, 'checkpoint_base', getattr(self.conf, 'checkpoints', '/tmp'))}/bronze\"\n",
    "        self.checkpoint_dir = checkpoint_dir or f\"{base_chk}/{self.table_name.replace('.', '_')}\"\n",
    "        self.spark: SparkSession = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "        # Ensure UC context\n",
    "        self._bootstrap_uc()\n",
    "\n",
    "        # Explicit schema to avoid drift (strings vs ints)\n",
    "        self.schema = self._build_schema()\n",
    "\n",
    "    # --------------------- UC bootstrap & table ---------------------\n",
    "    def _bootstrap_uc(self) -> None:\n",
    "        self.spark.sql(f\"CREATE CATALOG IF NOT EXISTS {self.catalog}\")\n",
    "        self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {self.catalog}.{self.db_name}\")\n",
    "        self.spark.sql(f\"USE CATALOG {self.catalog}\")\n",
    "        self.spark.sql(f\"USE {self.db_name}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _raw_table_columns_sql() -> str:\n",
    "        cols = [f\"V{i:02d} INT\" for i in range(96)]\n",
    "        return \",\\n                \".join(cols)\n",
    "\n",
    "    def _create_raw_table_if_not_exists(self) -> None:\n",
    "        cols96 = self._raw_table_columns_sql()\n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.table_fqn} (\n",
    "                NB_SCATS_SITE INT,\n",
    "                QT_INTERVAL_COUNT STRING,\n",
    "                NB_DETECTOR INT,\n",
    "                {cols96},\n",
    "                NM_REGION STRING,\n",
    "                CT_RECORDS INT,\n",
    "                QT_VOLUME_24HOUR INT,\n",
    "                CT_ALARM_24HOUR INT,\n",
    "                PartitionDate DATE,\n",
    "                load_time TIMESTAMP,\n",
    "                source_file STRING\n",
    "            )\n",
    "            USING DELTA\n",
    "            PARTITIONED BY (PartitionDate)\n",
    "        \"\"\")\n",
    "\n",
    "    def create_db(self) -> None:\n",
    "        self._bootstrap_uc()\n",
    "        self._create_raw_table_if_not_exists()\n",
    "\n",
    "    # --------------------- schema & transforms ---------------------\n",
    "    def _build_schema(self) -> T.StructType:\n",
    "        fields = [\n",
    "            T.StructField(\"NB_SCATS_SITE\", T.IntegerType(), True),\n",
    "            T.StructField(\"QT_INTERVAL_COUNT\", T.StringType(), True),\n",
    "            T.StructField(\"NB_DETECTOR\", T.IntegerType(), True),\n",
    "        ]\n",
    "        for i in range(96):\n",
    "            fields.append(T.StructField(f\"V{i:02d}\", T.IntegerType(), True))\n",
    "        fields += [\n",
    "            T.StructField(\"NM_REGION\", T.StringType(), True),\n",
    "            T.StructField(\"CT_RECORDS\", T.IntegerType(), True),\n",
    "            T.StructField(\"QT_VOLUME_24HOUR\", T.IntegerType(), True),\n",
    "            T.StructField(\"CT_ALARM_24HOUR\", T.IntegerType(), True),\n",
    "        ]\n",
    "        return T.StructType(fields)\n",
    "\n",
    "    @staticmethod\n",
    "    def _drop_if_exists(df: DataFrame, cols: List[str]) -> DataFrame:\n",
    "        for c in cols:\n",
    "            if c in df.columns:\n",
    "                df = df.drop(c)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _cast_volume_cols_int(df: DataFrame) -> DataFrame:\n",
    "        # Safe cast in case CSV parser brought them as strings\n",
    "        for i in range(96):\n",
    "            c = f\"V{i:02d}\"\n",
    "            if c in df.columns:\n",
    "                df = df.withColumn(c, col(c).cast(\"int\"))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_partition_date(df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        PartitionDate derived from QT_INTERVAL_COUNT.\n",
    "        Handles date or timestamp strings with several common formats.\n",
    "        \"\"\"\n",
    "        parsed_ts = coalesce(\n",
    "            to_timestamp(col(\"QT_INTERVAL_COUNT\"), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "            to_timestamp(col(\"QT_INTERVAL_COUNT\"), \"yyyy/MM/dd HH:mm:ss\"),\n",
    "            to_timestamp(col(\"QT_INTERVAL_COUNT\"), \"dd/MM/yyyy HH:mm:ss\"),\n",
    "            to_timestamp(col(\"QT_INTERVAL_COUNT\"), \"yyyy-MM-dd\"),\n",
    "            to_timestamp(col(\"QT_INTERVAL_COUNT\"), \"yyyy/MM/dd\"),\n",
    "            to_timestamp(col(\"QT_INTERVAL_COUNT\"))  # fallback\n",
    "        )\n",
    "        return df.withColumn(\"PartitionDate\", to_date(parsed_ts))\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_region(df: DataFrame) -> DataFrame:\n",
    "        # Normalize for downstream join with dim_region (built from REGION_MAP)\n",
    "        # - uppercase\n",
    "        # - trim leading/trailing spaces\n",
    "        # - remove internal whitespace so 'MC 1' → 'MC1'\n",
    "        return (df\n",
    "                .withColumn(\"NM_REGION\", F.upper(F.trim(col(\"NM_REGION\"))))\n",
    "                .withColumn(\"NM_REGION\", F.regexp_replace(\"NM_REGION\", r\"\\s+\", \"\")))\n",
    "\n",
    "    def _order_columns(self, df: DataFrame) -> DataFrame:\n",
    "        ordered = (\n",
    "            [\"NB_SCATS_SITE\", \"QT_INTERVAL_COUNT\", \"NB_DETECTOR\"]\n",
    "            + [f\"V{i:02d}\" for i in range(96)]\n",
    "            + [\"NM_REGION\", \"CT_RECORDS\", \"QT_VOLUME_24HOUR\", \"CT_ALARM_24HOUR\",\n",
    "               \"PartitionDate\", \"load_time\", \"source_file\"]\n",
    "        )\n",
    "        # keep only those that exist (defensive)\n",
    "        existing = [c for c in ordered if c in df.columns]\n",
    "        return df.select(*existing)\n",
    "\n",
    "    # --------------------- batch load ---------------------\n",
    "    def batch_load(self, start_date: str = \"2025-05-01\", end_date: str = \"2025-05-31\") -> None:\n",
    "        print(f\"📦 Batch load: {start_date} → {end_date} from {self.landing_zone}\")\n",
    "        try:\n",
    "            start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "            end = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
    "        except Exception:\n",
    "            raise ValueError(\"Dates must be in YYYY-MM-DD format\")\n",
    "\n",
    "        if end < start:\n",
    "            raise ValueError(\"end_date must be >= start_date\")\n",
    "\n",
    "        total_rows = 0\n",
    "        bad_records_path = f\"{self.checkpoint_dir}/bad_records\"\n",
    "\n",
    "        for delta in range((end - start).days + 1):\n",
    "            day = start + timedelta(days=delta)\n",
    "            filename = f\"VSDATA_{day.strftime('%Y%m%d')}.csv\"\n",
    "            path = f\"{self.landing_zone}/{filename}\"\n",
    "            try:\n",
    "                df = (\n",
    "                    self.spark.read\n",
    "                        .option(\"header\", True)\n",
    "                        .option(\"inferSchema\", False)\n",
    "                        .option(\"columnNameOfCorruptRecord\", \"_rescued_data\")\n",
    "                        .option(\"badRecordsPath\", bad_records_path)\n",
    "                        .schema(self.schema)\n",
    "                        .csv(path)\n",
    "                )\n",
    "\n",
    "                bad_cnt = 0\n",
    "                if \"_rescued_data\" in df.columns:\n",
    "                    bad_cnt = df.where(F.col(\"_rescued_data\").isNotNull()).limit(1).count()\n",
    "                if bad_cnt:\n",
    "                    print(f\"  ⚠️ {filename}: malformed rows detected (details under {bad_records_path})\")\n",
    "\n",
    "                df = (\n",
    "                    df.transform(self._drop_if_exists, [\"_rescued_data\"])\n",
    "                      .transform(self._cast_volume_cols_int)\n",
    "                      .transform(self._parse_partition_date)\n",
    "                      .transform(self._normalize_region)\n",
    "                      .withColumn(\"load_time\", F.current_timestamp())\n",
    "                      .withColumn(\"source_file\", F.col(\"_metadata.file_path\"))\n",
    "                )\n",
    "                df = self._order_columns(df)\n",
    "\n",
    "                (\n",
    "                    df.write.format(\"delta\")\n",
    "                      .mode(\"append\")\n",
    "                      .option(\"mergeSchema\", \"false\")\n",
    "                      .partitionBy(\"PartitionDate\")\n",
    "                      .saveAsTable(self.table_fqn)\n",
    "                )\n",
    "\n",
    "                count = df.count()\n",
    "                total_rows += count\n",
    "                print(f\"  ✅ {filename}: {count} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ {filename}: failed — {e}\")\n",
    "                raise\n",
    "\n",
    "        print(f\"Done. Total rows appended: {total_rows}\")\n",
    "\n",
    "    # --------------------- streaming load (Auto Loader) ---------------------\n",
    "    def stream_load(self, file_pattern: str = \"VSDATA_202506*.csv\", trigger_once: bool = True, reset_checkpoint: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Auto Loader over matching files, e.g. 'VSDATA_202508*.csv'.\n",
    "        - trigger_once=True  -> one micro-batch then stop\n",
    "        - trigger_once=False -> availableNow (ingest all available then stop)\n",
    "        \"\"\"\n",
    "        print(f\"🌊 Streaming load for pattern {file_pattern}\")\n",
    "        stream_path = f\"{self.landing_zone}/{file_pattern}\"\n",
    "        stream_chk = f\"{self.checkpoint_dir}/streaming\"\n",
    "        schema_loc = f\"{stream_chk}/schema\"\n",
    "\n",
    "        if reset_checkpoint:\n",
    "            print(f\"🧹 Cleaning checkpoint: {stream_chk}\")\n",
    "            try:\n",
    "                dbutils.fs.rm(stream_chk, recurse=True)\n",
    "                print(\"  ✅ Checkpoint cleared.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ Could not clear checkpoint ({e}). Continuing...\")\n",
    "\n",
    "        reader = (self.spark.readStream\n",
    "                    .format(\"cloudFiles\")\n",
    "                    .option(\"cloudFiles.format\", \"csv\")\n",
    "                    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                    .option(\"cloudFiles.schemaLocation\", schema_loc)\n",
    "                    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "                    .option(\"header\", True)\n",
    "                    .load(stream_path))\n",
    "\n",
    "        stream_df = (reader\n",
    "                     .transform(self._drop_if_exists, [\"_rescued_data\"])\n",
    "                     .transform(self._cast_volume_cols_int)\n",
    "                     .transform(self._parse_partition_date)\n",
    "                     .transform(self._normalize_region)\n",
    "                     .withColumn(\"load_time\", current_timestamp())\n",
    "                     .withColumn(\"source_file\", col(\"_metadata.file_path\")))\n",
    "        stream_df = self._order_columns(stream_df)\n",
    "\n",
    "        writer = (stream_df.writeStream\n",
    "                    .format(\"delta\")\n",
    "                    .option(\"checkpointLocation\", stream_chk)\n",
    "                    .option(\"mergeSchema\", \"false\")\n",
    "                    .partitionBy(\"PartitionDate\")\n",
    "                    .outputMode(\"append\"))\n",
    "\n",
    "        if trigger_once:\n",
    "            query = writer.trigger(once=True).toTable(self.table_fqn)\n",
    "        else:\n",
    "            query = writer.trigger(availableNow=True).toTable(self.table_fqn)\n",
    "\n",
    "        query.awaitTermination()\n",
    "        print(\"✅ Streaming load completed.\")\n",
    "\n",
    "    # --------------------- validation / maintenance ---------------------\n",
    "    def validate_table(self) -> None:\n",
    "        print(f\"🔎 Validating {self.table_fqn} ...\")\n",
    "        try:\n",
    "            df = self.spark.table(self.table_fqn)\n",
    "            total = df.count()\n",
    "            parts = df.select(\"PartitionDate\").distinct().orderBy(\"PartitionDate\").collect()\n",
    "            first = parts[0][\"PartitionDate\"] if parts else None\n",
    "            last  = parts[-1][\"PartitionDate\"] if parts else None\n",
    "            print(f\"✅ {self.table_fqn}: {total} rows across {len(parts)} partitions \"\n",
    "                  f\"(first={first}, last={last}).\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Validation failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def vacuum_optimize(self, zorder_cols=None):\n",
    "        zorder_cols = zorder_cols or [\"PartitionDate\", \"NB_SCATS_SITE\", \"NB_DETECTOR\"]\n",
    "        self.spark.sql(f\"OPTIMIZE {self.table_fqn} ZORDER BY ({', '.join(zorder_cols)})\")\n",
    "        print(\"Optimize + ZORDER done.\")\n",
    "\n",
    "\n",
    "# --------------------- example run (bronze) ---------------------\n",
    "conf = Config()  # uses ENV/widgets from 1_config.py\n",
    "bronze = LoadRawTraffic(catalog=conf.catalog, table_name=conf.bronze_table, env=conf.env)\n",
    "\n",
    "bronze.create_db()\n",
    "# Historical backfill (adjust dates as needed)\n",
    "bronze.batch_load(start_date=\"2025-05-01\", end_date=\"2025-05-31\")\n",
    "# Ingest a set of new drops via Auto Loader\n",
    "bronze.stream_load(file_pattern=\"VSDATA_202506*.csv\", trigger_once=True)\n",
    "bronze.validate_table()\n",
    "print(\"✅ bronze load/validate OK\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4_bronze_loader.py",
   "widgets": {
    "ENV": {
     "currentValue": "dev",
     "nuid": "4c57c176-964c-4995-b3c0-b80430633ef0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "qa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "dev",
        "qa"
       ]
      }
     }
    },
    "METASTORE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "5dd4dfed-06c8-4aaa-a4a9-9e8567e7adcf",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "STORAGE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "512f814c-4e17-4be4-8de1-aa1f5af31b31",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
