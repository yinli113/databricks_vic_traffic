{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8689485b-cece-46db-946e-a785760f4430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./1_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72cc9487-53ad-4fa0-ad4c-a011e9048310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- inline params (dev/qa) ---\n",
    "import os, importlib\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"ENV\", \"dev\", [\"dev\", \"qa\"], \"Environment\")\n",
    "    dbutils.widgets.dropdown(\"STORAGE_ACCOUNT\", \"trafficsa2\", [\"trafficsa2\", \"trafficsaqa\"], \"Storage account\")\n",
    "    dbutils.widgets.text(\"METASTORE_ACCOUNT\", \"trafficsa2\", \"Metastore account\")\n",
    "    os.environ[\"ENV\"] = dbutils.widgets.get(\"ENV\").strip().lower()\n",
    "    os.environ[\"STORAGE_ACCOUNT\"] = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\n",
    "    os.environ[\"METASTORE_ACCOUNT\"] = (dbutils.widgets.get(\"METASTORE_ACCOUNT\") or os.environ[\"STORAGE_ACCOUNT\"]).strip()\n",
    "except NameError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f6a4798-d586-48b2-a331-92d051dfbdb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from typing import Optional, List\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_timestamp, to_date, input_file_name,\n",
    "    to_timestamp, coalesce\n",
    ")\n",
    "\n",
    "class LoadRawTraffic:\n",
    "    \"\"\"\n",
    "    Bronze loader for SCATS 'Traffic Signal Volume' CSVs.\n",
    "    Writes to {catalog}.{db}.raw_traffic (schema created in 2_setup.py).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, catalog: str, table_name: str, checkpoint_dir: Optional[str] = None, env: Optional[str] = None):\n",
    "        self.conf = Config(env or catalog)\n",
    "        self.catalog = catalog\n",
    "        self.db_name = self.conf.db_name\n",
    "        self.landing_zone = self.conf.raw_data_path\n",
    "        self.table_name = table_name\n",
    "        self.table_fqn = self.conf.table_fqn(table_name)\n",
    "        base_chk = f\"{self.conf.checkpoint_base}/bronze\"\n",
    "        self.checkpoint_dir = checkpoint_dir or f\"{base_chk}/{self.table_name.replace('.', '_')}\"\n",
    "        self.spark: SparkSession = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "        # Ensure UC context\n",
    "        self._bootstrap_uc()\n",
    "\n",
    "        # Explicit schema to avoid drift (strings vs ints)\n",
    "        self.schema = self._build_schema()\n",
    "\n",
    "    # --------------------- UC bootstrap & table ---------------------\n",
    "    def _bootstrap_uc(self) -> None:\n",
    "        self.spark.sql(f\"CREATE CATALOG IF NOT EXISTS {self.catalog}\")\n",
    "        self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {self.catalog}.{self.db_name}\")\n",
    "        self.spark.sql(f\"USE CATALOG {self.catalog}\")\n",
    "        self.spark.sql(f\"USE {self.db_name}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _raw_table_columns_sql() -> str:\n",
    "        cols = [f\"V{i:02d} INT\" for i in range(96)]\n",
    "        return \",\\n                \".join(cols)\n",
    "\n",
    "    def _create_raw_table_if_not_exists(self) -> None:\n",
    "        cols96 = self._raw_table_columns_sql()\n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.table_fqn} (\n",
    "                NB_SCATS_SITE INT,\n",
    "                QT_INTERVAL_COUNT STRING,\n",
    "                NB_DETECTOR INT,\n",
    "                {cols96},\n",
    "                NM_REGION STRING,\n",
    "                CT_RECORDS INT,\n",
    "                QT_VOLUME_24HOUR INT,\n",
    "                CT_ALARM_24HOUR INT,\n",
    "                PartitionDate DATE,\n",
    "                load_time TIMESTAMP,\n",
    "                source_file STRING\n",
    "            )\n",
    "            USING DELTA\n",
    "            PARTITIONED BY (PartitionDate)\n",
    "        \"\"\")\n",
    "\n",
    "    def create_db(self) -> None:\n",
    "        self._bootstrap_uc()\n",
    "        self._create_raw_table_if_not_exists()\n",
    "\n",
    "    # --------------------- schema & transforms ---------------------\n",
    "    def _build_schema(self) -> T.StructType:\n",
    "        fields = [\n",
    "            T.StructField(\"NB_SCATS_SITE\", T.IntegerType(), True),\n",
    "            T.StructField(\"QT_INTERVAL_COUNT\", T.StringType(), True),\n",
    "            T.StructField(\"NB_DETECTOR\", T.IntegerType(), True),\n",
    "        ]\n",
    "        for i in range(96):\n",
    "            fields.append(T.StructField(f\"V{i:02d}\", T.IntegerType(), True))\n",
    "        fields += [\n",
    "            T.StructField(\"NM_REGION\", T.StringType(), True),\n",
    "            T.StructField(\"CT_RECORDS\", T.IntegerType(), True),\n",
    "            T.StructField(\"QT_VOLUME_24HOUR\", T.IntegerType(), True),\n",
    "            T.StructField(\"CT_ALARM_24HOUR\", T.IntegerType(), True),\n",
    "        ]\n",
    "        return T.StructType(fields)\n",
    "\n",
    "    @staticmethod\n",
    "    def _drop_if_exists(df: DataFrame, cols: List[str]) -> DataFrame:\n",
    "        for c in cols:\n",
    "            if c in df.columns:\n",
    "                df = df.drop(c)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _cast_volume_cols_int(df: DataFrame) -> DataFrame:\n",
    "        # Safe cast in case CSV parser brought them as strings\n",
    "        for i in range(96):\n",
    "            c = f\"V{i:02d}\"\n",
    "            if c in df.columns:\n",
    "                df = df.withColumn(c, col(c).cast(\"int\"))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_partition_date(df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        PartitionDate derived from QT_INTERVAL_COUNT.\n",
    "        Handles date or timestamp strings with several common formats.\n",
    "        \"\"\"\n",
    "        parsed_ts = coalesce(\n",
    "            to_timestamp(col(\"QT_INTERVAL_COUNT\"), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "            to_timestamp(col(\"QT_INTERVAL_COUNT\"), \"yyyy/MM/dd HH:mm:ss\"),\n",
    "            to_timestamp(col(\"QT_INTERVAL_COUNT\"), \"dd/MM/yyyy HH:mm:ss\"),\n",
    "            to_timestamp(col(\"QT_INTERVAL_COUNT\"), \"yyyy-MM-dd\"),\n",
    "            to_timestamp(col(\"QT_INTERVAL_COUNT\"), \"yyyy/MM/dd\"),\n",
    "            to_timestamp(col(\"QT_INTERVAL_COUNT\"))  # fallback\n",
    "        )\n",
    "        return df.withColumn(\"PartitionDate\", to_date(parsed_ts))\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_region(df: DataFrame) -> DataFrame:\n",
    "        return df.withColumn(\"NM_REGION\", F.upper(F.trim(col(\"NM_REGION\"))))\n",
    "\n",
    "    def _order_columns(self, df: DataFrame) -> DataFrame:\n",
    "        ordered = (\n",
    "            [\"NB_SCATS_SITE\", \"QT_INTERVAL_COUNT\", \"NB_DETECTOR\"]\n",
    "            + [f\"V{i:02d}\" for i in range(96)]\n",
    "            + [\"NM_REGION\", \"CT_RECORDS\", \"QT_VOLUME_24HOUR\", \"CT_ALARM_24HOUR\",\n",
    "               \"PartitionDate\", \"load_time\", \"source_file\"]\n",
    "        )\n",
    "        # keep only those that exist (defensive)\n",
    "        existing = [c for c in ordered if c in df.columns]\n",
    "        return df.select(*existing)\n",
    "\n",
    "    # --------------------- batch load ---------------------\n",
    "    def batch_load(self, start_date: str = \"2025-05-01\", end_date: str = \"2025-05-01\") -> None:\n",
    "        \"\"\"\n",
    "        Backfill daily CSVs in [start_date, end_date], inclusive.\n",
    "        Expects files named VSDATA_YYYYMMDD.csv under self.landing_zone.\n",
    "        \"\"\"\n",
    "        print(f\"📦 Batch load: {start_date} → {end_date} from {self.landing_zone}\")\n",
    "        try:\n",
    "            start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "            end = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
    "        except Exception:\n",
    "            raise ValueError(\"Dates must be in YYYY-MM-DD format\")\n",
    "\n",
    "        if end < start:\n",
    "            raise ValueError(\"end_date must be >= start_date\")\n",
    "\n",
    "        total_rows = 0\n",
    "        for delta in range((end - start).days + 1):\n",
    "            day = start + timedelta(days=delta)\n",
    "            filename = f\"VSDATA_{day.strftime('%Y%m%d')}.csv\"\n",
    "            path = f\"{self.landing_zone}/{filename}\"\n",
    "            try:\n",
    "                df = (self.spark.read\n",
    "                        .option(\"header\", True)\n",
    "                        .option(\"inferSchema\", False)\n",
    "                        .schema(self.schema)\n",
    "                        .csv(path))\n",
    "\n",
    "                if \"QT_INTERVAL_COUNT\" not in df.columns:\n",
    "                    raise ValueError(\"Column QT_INTERVAL_COUNT is missing in the file.\")\n",
    "\n",
    "                df = (df.transform(self._drop_if_exists, [\"_rescued_data\"])\n",
    "                        .transform(self._cast_volume_cols_int)\n",
    "                        .transform(self._parse_partition_date)\n",
    "                        .transform(self._normalize_region)\n",
    "                        .withColumn(\"load_time\", current_timestamp())\n",
    "                        .withColumn(\"source_file\", input_file_name()))\n",
    "                df = self._order_columns(df)\n",
    "\n",
    "                (df.write.format(\"delta\")\n",
    "                    .mode(\"append\")\n",
    "                    .option(\"mergeSchema\", \"false\")\n",
    "                    .partitionBy(\"PartitionDate\")\n",
    "                    .saveAsTable(self.table_fqn))\n",
    "\n",
    "                count = df.count()\n",
    "                total_rows += count\n",
    "                print(f\"  ✅ {filename}: {count} rows\")\n",
    "            except AnalysisException as e:\n",
    "                print(f\"  ⚠️ {filename}: skipped (AnalysisException: {str(e).splitlines()[0]})\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ {filename}: failed — {e}\")\n",
    "                raise\n",
    "        print(f\"Done. Total rows appended: {total_rows}\")\n",
    "\n",
    "    # --------------------- streaming load (Auto Loader) ---------------------\n",
    "    def stream_load(self, file_pattern: str = \"VSDATA_202506*.csv\", trigger_once: bool = True, reset_checkpoint: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Auto Loader over matching files, e.g. 'VSDATA_202508*.csv'.\n",
    "        - trigger_once=True  -> one micro-batch then stop\n",
    "        - trigger_once=False -> availableNow (ingest all available then stop)\n",
    "        \"\"\"\n",
    "        print(f\"🌊 Streaming load for pattern {file_pattern}\")\n",
    "        stream_path = f\"{self.landing_zone}/{file_pattern}\"\n",
    "        stream_chk = f\"{self.checkpoint_dir}/streaming\"\n",
    "        schema_loc = f\"{stream_chk}/schema\"\n",
    "\n",
    "        if reset_checkpoint:\n",
    "            print(f\"🧹 Cleaning checkpoint: {stream_chk}\")\n",
    "            try:\n",
    "                dbutils.fs.rm(stream_chk, recurse=True)\n",
    "                print(\"  ✅ Checkpoint cleared.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ Could not clear checkpoint ({e}). Continuing...\")\n",
    "\n",
    "        reader = (self.spark.readStream\n",
    "                    .format(\"cloudFiles\")\n",
    "                    .option(\"cloudFiles.format\", \"csv\")\n",
    "                    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                    .option(\"cloudFiles.schemaLocation\", schema_loc)\n",
    "                    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "                    .option(\"header\", True)\n",
    "                    .load(stream_path))\n",
    "\n",
    "        stream_df = (reader\n",
    "                     .transform(self._drop_if_exists, [\"_rescued_data\"])\n",
    "                     .transform(self._cast_volume_cols_int)\n",
    "                     .transform(self._parse_partition_date)\n",
    "                     .transform(self._normalize_region)\n",
    "                     .withColumn(\"load_time\", current_timestamp())\n",
    "                     .withColumn(\"source_file\", col(\"_metadata.file_path\")))\n",
    "        stream_df = self._order_columns(stream_df)\n",
    "\n",
    "        writer = (stream_df.writeStream\n",
    "                    .format(\"delta\")\n",
    "                    .option(\"checkpointLocation\", stream_chk)\n",
    "                    .option(\"mergeSchema\", \"false\")\n",
    "                    .partitionBy(\"PartitionDate\")\n",
    "                    .outputMode(\"append\"))\n",
    "\n",
    "        if trigger_once:\n",
    "            query = writer.trigger(once=True).toTable(self.table_fqn)\n",
    "        else:\n",
    "            query = writer.trigger(availableNow=True).toTable(self.table_fqn)\n",
    "\n",
    "        query.awaitTermination()\n",
    "        print(\"✅ Streaming load completed.\")\n",
    "\n",
    "    # --------------------- validation / maintenance ---------------------\n",
    "    def validate_table(self) -> None:\n",
    "        print(f\"🔎 Validating {self.table_fqn} ...\")\n",
    "        try:\n",
    "            df = self.spark.table(self.table_fqn)\n",
    "            total = df.count()\n",
    "            parts = df.select(\"PartitionDate\").distinct().orderBy(\"PartitionDate\").collect()\n",
    "            first = parts[0][\"PartitionDate\"] if parts else None\n",
    "            last  = parts[-1][\"PartitionDate\"] if parts else None\n",
    "            print(f\"✅ {self.table_fqn}: {total} rows across {len(parts)} partitions \"\n",
    "                  f\"(first={first}, last={last}).\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Validation failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def vacuum_optimize(self, zorder_cols=None):\n",
    "        zorder_cols = zorder_cols or [\"PartitionDate\", \"NB_SCATS_SITE\", \"NB_DETECTOR\"]\n",
    "        self.spark.sql(f\"OPTIMIZE {self.table_fqn} ZORDER BY ({', '.join(zorder_cols)})\")\n",
    "        print(\"Optimize + ZORDER done.\")\n",
    "\n",
    "\n",
    "# --------------------- example run (bronze) ---------------------\n",
    "conf = Config()  # uses ENV/widgets from 1_config.py\n",
    "bronze = LoadRawTraffic(catalog=conf.catalog, table_name=conf.bronze_table, env=conf.env)\n",
    "\n",
    "bronze.create_db()\n",
    "# Historical backfill (adjust dates as needed)\n",
    "bronze.batch_load(start_date=\"2025-05-01\", end_date=\"2025-05-01\")\n",
    "# Ingest a set of new drops via Auto Loader\n",
    "bronze.stream_load(file_pattern=\"VSDATA_202506*.csv\", trigger_once=True)\n",
    "bronze.validate_table()\n",
    "print(\"✅ bronze load/validate OK\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4_bronze_loader.py",
   "widgets": {
    "ENV": {
     "currentValue": "dev",
     "nuid": "4c57c176-964c-4995-b3c0-b80430633ef0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "qa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "dev",
        "qa"
       ]
      }
     }
    },
    "METASTORE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "5dd4dfed-06c8-4aaa-a4a9-9e8567e7adcf",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "STORAGE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "512f814c-4e17-4be4-8de1-aa1f5af31b31",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
