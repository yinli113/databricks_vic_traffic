{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ffe0aa-7ef3-4ae8-baf7-84df83d022f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./1_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81024517-c083-4230-81bc-aa0355e0fd42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, importlib\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"ENV\", \"dev\", [\"dev\", \"qa\"], \"Environment\")\n",
    "    dbutils.widgets.dropdown(\"STORAGE_ACCOUNT\", \"trafficsa2\", [\"trafficsa2\", \"trafficsaqa\"], \"Storage account\")\n",
    "    dbutils.widgets.text(\"METASTORE_ACCOUNT\", \"trafficsa2\", \"Metastore account\")\n",
    "    os.environ[\"ENV\"] = dbutils.widgets.get(\"ENV\").strip().lower()\n",
    "    os.environ[\"STORAGE_ACCOUNT\"] = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\n",
    "    os.environ[\"METASTORE_ACCOUNT\"] = (dbutils.widgets.get(\"METASTORE_ACCOUNT\") or os.environ[\"STORAGE_ACCOUNT\"]).strip()\n",
    "except NameError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4af11ad3-a7de-4a50-ac01-7008061d358e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %run ./1_config.py\n",
    "# MAGIC %run ./2_setup.py\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import re\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Gold Builder\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "class GoldBuilder:\n",
    "    \"\"\"\n",
    "    Builds Gold star-schema tables from Silver:\n",
    "      - fact_traffic_15min   (Region/Site/Detector x 15-minute interval)\n",
    "      - fact_daily_summary   (Detector x Day)\n",
    "    Also keeps:\n",
    "      - dim_time             (Date, Hour, Year, Month, DayOfWeek, WeekdayFlag)\n",
    "      - dim_detector         (NB_DETECTOR, NB_SCATS_SITE, NM_REGION, suburb via region_lookup)\n",
    "\n",
    "    Assumptions in Silver:\n",
    "      NB_SCATS_SITE, NB_DETECTOR, QT_INTERVAL_COUNT, V00..V95,\n",
    "      NM_REGION, CT_RECORDS, QT_VOLUME_24HOUR, CT_ALARM_24HOUR,\n",
    "      ReadingDate (DATE), ReadingTs (TIMESTAMP), NM_REGION_NORM, PartitionDate, load_time, source_file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conf_obj: \"Config\" = None):\n",
    "        self.conf = conf_obj or conf\n",
    "        self.spark: SparkSession = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "        self.catalog = self.conf.catalog\n",
    "        self.db_name = self.conf.db_name\n",
    "\n",
    "        # FQNs\n",
    "        self.silver_fqn = self.conf.table_fqn(self.conf.silver_table)\n",
    "        self.dim_time_fqn = f\"{self.catalog}.{self.db_name}.dim_time\"\n",
    "        self.dim_detector_fqn = f\"{self.catalog}.{self.db_name}.dim_detector\"\n",
    "        self.region_lookup_fqn = f\"{self.catalog}.{self.db_name}.region_lookup\"\n",
    "\n",
    "        self.fact_15_fqn = f\"{self.catalog}.{self.db_name}.fact_traffic_15min\"\n",
    "        self.fact_daily_fqn = f\"{self.catalog}.{self.db_name}.fact_daily_summary\"\n",
    "\n",
    "        self._bootstrap_uc()\n",
    "        self._ensure_fact_tables()\n",
    "\n",
    "    # ---------------------------- bootstrap ----------------------------\n",
    "    def _bootstrap_uc(self) -> None:\n",
    "        self.spark.sql(f\"CREATE CATALOG IF NOT EXISTS {self.catalog}\")\n",
    "        self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {self.catalog}.{self.db_name}\")\n",
    "        self.spark.sql(f\"USE CATALOG {self.catalog}\")\n",
    "        self.spark.sql(f\"USE {self.db_name}\")\n",
    "\n",
    "    def _ensure_fact_tables(self) -> None:\n",
    "        # Schemas aligned to what we created in setup\n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.fact_15_fqn} (\n",
    "                TimeKey BIGINT,\n",
    "                DetectorKey STRING,\n",
    "                SiteKey STRING,\n",
    "                RegionKey STRING,\n",
    "                IntervalStartTime TIMESTAMP,\n",
    "                Volume BIGINT,\n",
    "                Year INT,\n",
    "                Month INT\n",
    "            )\n",
    "            USING DELTA\n",
    "            PARTITIONED BY (Year, Month)\n",
    "        \"\"\")\n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.fact_daily_fqn} (\n",
    "                DateKey INT,\n",
    "                DetectorKey STRING,\n",
    "                SiteKey STRING,\n",
    "                RegionKey STRING,\n",
    "                ReadingDate DATE,\n",
    "                TotalVolume BIGINT,\n",
    "                AlarmCount BIGINT,\n",
    "                IntervalsWithData INT\n",
    "            )\n",
    "            USING DELTA\n",
    "        \"\"\")\n",
    "\n",
    "    # ---------------------------- helpers ------------------------------\n",
    "    @staticmethod\n",
    "    def _hash_key(cols: list) -> F.Column:\n",
    "        return F.sha2(F.concat_ws(\"||\", *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in cols]), 256)\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_vcols(df: DataFrame) -> list:\n",
    "        \"\"\"Return ordered Vxx column names, case-insensitive, sorted by xx (0..95).\"\"\"\n",
    "        found = []\n",
    "        for c in df.columns:\n",
    "            m = re.fullmatch(r'v(\\d{2})', c, re.IGNORECASE)\n",
    "            if m:\n",
    "                found.append((c, int(m.group(1))))\n",
    "        found = sorted(found, key=lambda x: x[1])\n",
    "        return [name for name, _ in found]\n",
    "\n",
    "    @staticmethod\n",
    "    def _melt_15min(df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Wide V00..V95 -> long rows with IntervalIndex, Volume, IntervalStartTime.\n",
    "        Uses posexplode_outer with proper alias of two columns to avoid UDTF alias mismatch.\n",
    "        \"\"\"\n",
    "        vcols = GoldBuilder._find_vcols(df)\n",
    "        if not vcols:\n",
    "            raise ValueError(\"No Vxx (V00..V95) columns found in Silver.\")\n",
    "\n",
    "        # Project identifiers + all Vxx first\n",
    "        base = df.select(\"NB_SCATS_SITE\", \"NB_DETECTOR\", \"NM_REGION_NORM\", \"ReadingDate\", *vcols)\n",
    "\n",
    "        # Build exploded rows: (IntervalIndex, Volume)\n",
    "        exploded = base.select(\n",
    "            \"NB_SCATS_SITE\", \"NB_DETECTOR\", \"NM_REGION_NORM\", \"ReadingDate\",\n",
    "            F.posexplode_outer(\n",
    "                F.array(*[F.col(c).cast(\"long\") for c in vcols])\n",
    "            ).alias(\"IntervalIndex\", \"Volume\")\n",
    "        )\n",
    "\n",
    "        # IntervalStartTime = midnight + IntervalIndex*15 minutes (use timestampadd for ANSI safety)\n",
    "        midnight = F.to_timestamp(F.concat_ws(\" \", F.date_format(F.col(\"ReadingDate\"), \"yyyy-MM-dd\"), F.lit(\"00:00:00\")))\n",
    "        out = (exploded\n",
    "               .withColumn(\"Midnight\", midnight)\n",
    "               .withColumn(\"IntervalStartTime\", F.expr(\"timestampadd(MINUTE, IntervalIndex * 15, Midnight)\"))\n",
    "               .drop(\"Midnight\"))\n",
    "        return out\n",
    "\n",
    "    # ---------------------------- dimensions ---------------------------\n",
    "    def _upsert_dim_time(self, intervals_df: DataFrame) -> None:\n",
    "        \"\"\"Insert missing (Date, Hour) rows into dim_time based on IntervalStartTime.\"\"\"\n",
    "        dim = (intervals_df\n",
    "               .select(\n",
    "                   F.to_date(\"IntervalStartTime\").alias(\"Date\"),\n",
    "                   F.hour(\"IntervalStartTime\").alias(\"Hour\"),\n",
    "                   F.year(\"IntervalStartTime\").alias(\"Year\"),\n",
    "                   F.month(\"IntervalStartTime\").alias(\"Month\"),\n",
    "                   F.date_format(\"IntervalStartTime\",\"E\").alias(\"DayOfWeek\"),\n",
    "                   F.when(F.dayofweek(\"IntervalStartTime\").isin(1,7), F.lit(False))  # Sun=1, Sat=7\n",
    "                     .otherwise(F.lit(True)).alias(\"WeekdayFlag\")\n",
    "               )\n",
    "               .dropDuplicates([\"Date\",\"Hour\"]))\n",
    "\n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.dim_time_fqn} (\n",
    "                Date DATE, Hour INT, Year INT, Month INT, DayOfWeek STRING, WeekdayFlag BOOLEAN\n",
    "            ) USING DELTA\n",
    "        \"\"\")\n",
    "        tgt = self.spark.table(self.dim_time_fqn).select(\"Date\",\"Hour\")\n",
    "        new_rows = dim.join(tgt, on=[\"Date\",\"Hour\"], how=\"left_anti\")\n",
    "        if new_rows.count() > 0:\n",
    "            new_rows.write.mode(\"append\").format(\"delta\").saveAsTable(self.dim_time_fqn)\n",
    "\n",
    "    def _upsert_dim_detector(self, silver_df: DataFrame) -> None:\n",
    "        \"\"\"Ensure dim_detector has latest detector/site/region + suburb via region_lookup.\"\"\"\n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.dim_detector_fqn} (\n",
    "                NB_DETECTOR INT, NB_SCATS_SITE INT, NM_REGION STRING, suburb STRING\n",
    "            ) USING DELTA\n",
    "        \"\"\")\n",
    "        base = (silver_df\n",
    "                .select(\n",
    "                    F.col(\"NB_DETECTOR\").cast(\"int\").alias(\"NB_DETECTOR\"),\n",
    "                    F.col(\"NB_SCATS_SITE\").cast(\"int\").alias(\"NB_SCATS_SITE\"),\n",
    "                    F.col(\"NM_REGION_NORM\").alias(\"NM_REGION\")\n",
    "                ).dropDuplicates())\n",
    "\n",
    "        # Left join lookup to fill suburb\n",
    "        try:\n",
    "            lk = self.spark.table(self.region_lookup_fqn).select(\n",
    "                F.upper(F.trim(F.col(\"NM_REGION\"))).alias(\"NM_REGION\"),\n",
    "                F.col(\"SUBURB\").alias(\"suburb\")\n",
    "            )\n",
    "            base = base.join(lk, \"NM_REGION\", \"left\")\n",
    "        except AnalysisException:\n",
    "            base = base.withColumn(\"suburb\", F.lit(None).cast(\"string\"))\n",
    "\n",
    "        tgt = self.spark.table(self.dim_detector_fqn).select(\"NB_DETECTOR\",\"NB_SCATS_SITE\",\"NM_REGION\").dropDuplicates()\n",
    "        new_rows = base.join(tgt, on=[\"NB_DETECTOR\",\"NB_SCATS_SITE\",\"NM_REGION\"], how=\"left_anti\")\n",
    "        if new_rows.count() > 0:\n",
    "            new_rows.write.mode(\"append\").format(\"delta\").saveAsTable(self.dim_detector_fqn)\n",
    "\n",
    "    # ---------------------------- facts -------------------------------\n",
    "    def _build_fact_15min(self, silver_df: DataFrame, since_load_time: str = None, full_rebuild: bool = False) -> int:\n",
    "        src = silver_df\n",
    "        if since_load_time and not full_rebuild:\n",
    "            src = src.where(F.col(\"load_time\") >= F.to_timestamp(F.lit(since_load_time)))\n",
    "\n",
    "        melted = self._melt_15min(src).cache()\n",
    "\n",
    "        fact_src = (melted\n",
    "                    .withColumn(\"RegionKey\", self._hash_key([\"NM_REGION_NORM\"]))\n",
    "                    .withColumn(\"SiteKey\",   self._hash_key([\"NB_SCATS_SITE\"]))\n",
    "                    .withColumn(\"DetectorKey\", self._hash_key([\"NB_SCATS_SITE\",\"NB_DETECTOR\"]))\n",
    "                    .withColumn(\"TimeKey\", F.date_format(\"IntervalStartTime\",\"yyyyMMddHHmm\").cast(\"bigint\"))\n",
    "                    .withColumn(\"Year\", F.year(\"IntervalStartTime\"))\n",
    "                    .withColumn(\"Month\", F.month(\"IntervalStartTime\"))\n",
    "                    .select(\"TimeKey\",\"DetectorKey\",\"SiteKey\",\"RegionKey\",\"IntervalStartTime\",\"Volume\",\"Year\",\"Month\"))\n",
    "\n",
    "        if full_rebuild:\n",
    "            self.spark.sql(f\"TRUNCATE TABLE {self.fact_15_fqn}\")\n",
    "            fact_src.write.mode(\"append\").format(\"delta\").partitionBy(\"Year\",\"Month\").saveAsTable(self.fact_15_fqn)\n",
    "            rows = fact_src.count()\n",
    "            melted.unpersist()\n",
    "            return rows\n",
    "        else:\n",
    "            fact_src.createOrReplaceTempView(\"__fact15_src\")\n",
    "            self.spark.sql(f\"\"\"\n",
    "                MERGE INTO {self.fact_15_fqn} AS tgt\n",
    "                USING __fact15_src AS src\n",
    "                  ON tgt.TimeKey = src.TimeKey AND tgt.DetectorKey = src.DetectorKey\n",
    "                WHEN MATCHED THEN UPDATE SET *\n",
    "                WHEN NOT MATCHED THEN INSERT *\n",
    "            \"\"\")\n",
    "            rows = melted.count()\n",
    "            melted.unpersist()\n",
    "            return rows\n",
    "\n",
    "    def _build_fact_daily(self, silver_df: DataFrame, since_load_time: str = None, full_rebuild: bool = False) -> int:\n",
    "        src = silver_df\n",
    "        if since_load_time and not full_rebuild:\n",
    "            src = src.where(F.col(\"load_time\") >= F.to_timestamp(F.lit(since_load_time)))\n",
    "\n",
    "        daily_src = (src\n",
    "                     .select(\n",
    "                         F.col(\"NB_SCATS_SITE\").cast(\"int\").alias(\"NB_SCATS_SITE\"),\n",
    "                         F.col(\"NB_DETECTOR\").cast(\"int\").alias(\"NB_DETECTOR\"),\n",
    "                         F.col(\"NM_REGION_NORM\").alias(\"NM_REGION\"),\n",
    "                         F.col(\"ReadingDate\").alias(\"ReadingDate\"),\n",
    "                         F.col(\"CT_RECORDS\").cast(\"int\").alias(\"IntervalsWithData\"),\n",
    "                         F.col(\"QT_VOLUME_24HOUR\").cast(\"long\").alias(\"TotalVolume\"),\n",
    "                         F.col(\"CT_ALARM_24HOUR\").cast(\"long\").alias(\"AlarmCount\")\n",
    "                     )\n",
    "                     .dropna(subset=[\"ReadingDate\",\"NB_DETECTOR\",\"NB_SCATS_SITE\"]))\n",
    "\n",
    "        fact_src = (daily_src\n",
    "                    .withColumn(\"RegionKey\", self._hash_key([\"NM_REGION\"]))\n",
    "                    .withColumn(\"SiteKey\",   self._hash_key([\"NB_SCATS_SITE\"]))\n",
    "                    .withColumn(\"DetectorKey\", self._hash_key([\"NB_SCATS_SITE\",\"NB_DETECTOR\"]))\n",
    "                    .withColumn(\"DateKey\", F.date_format(\"ReadingDate\",\"yyyyMMdd\").cast(\"int\"))\n",
    "                    .select(\"DateKey\",\"DetectorKey\",\"SiteKey\",\"RegionKey\",\"ReadingDate\",\"TotalVolume\",\"AlarmCount\",\"IntervalsWithData\"))\n",
    "\n",
    "        if full_rebuild:\n",
    "            self.spark.sql(f\"TRUNCATE TABLE {self.fact_daily_fqn}\")\n",
    "            fact_src.write.mode(\"append\").format(\"delta\").saveAsTable(self.fact_daily_fqn)\n",
    "            return fact_src.count()\n",
    "        else:\n",
    "            fact_src.createOrReplaceTempView(\"__factdaily_src\")\n",
    "            self.spark.sql(f\"\"\"\n",
    "                MERGE INTO {self.fact_daily_fqn} AS tgt\n",
    "                USING __factdaily_src AS src\n",
    "                  ON tgt.DateKey = src.DateKey AND tgt.DetectorKey = src.DetectorKey\n",
    "                WHEN MATCHED THEN UPDATE SET *\n",
    "                WHEN NOT MATCHED THEN INSERT *\n",
    "            \"\"\")\n",
    "            return fact_src.count()\n",
    "\n",
    "    # ---------------------------- public APIs -------------------------\n",
    "    def rebuild_all(self) -> None:\n",
    "        \"\"\"\n",
    "        Full rebuild: refresh dimensions, truncate facts, load everything from Silver.\n",
    "        \"\"\"\n",
    "        silver = self.spark.table(self.silver_fqn)\n",
    "\n",
    "        # Dimensions\n",
    "        melted = self._melt_15min(silver)\n",
    "        self._upsert_dim_time(melted)\n",
    "        self._upsert_dim_detector(silver)\n",
    "\n",
    "        # Facts\n",
    "        rows15 = self._build_fact_15min(silver, full_rebuild=True)\n",
    "        rowsD  = self._build_fact_daily(silver, full_rebuild=True)\n",
    "        print(f\"âœ… Full Gold rebuild complete: 15min={rows15}, daily={rowsD}\")\n",
    "\n",
    "    def incremental_upsert(self, since_load_time: str) -> None:\n",
    "        \"\"\"\n",
    "        Process only Silver rows with load_time >= since_load_time (ISO timestamp).\n",
    "        Keeps dimensions up to date.\n",
    "        \"\"\"\n",
    "        silver = self.spark.table(self.silver_fqn)\n",
    "\n",
    "        # Dimensions (only for new intervals)\n",
    "        melted = self._melt_15min(silver.where(F.col(\"load_time\") >= F.to_timestamp(F.lit(since_load_time))))\n",
    "        self._upsert_dim_time(melted)\n",
    "        self._upsert_dim_detector(silver.where(F.col(\"load_time\") >= F.to_timestamp(F.lit(since_load_time))))\n",
    "\n",
    "        rows15 = self._build_fact_15min(silver, since_load_time=since_load_time, full_rebuild=False)\n",
    "        rowsD  = self._build_fact_daily(silver, since_load_time=since_load_time, full_rebuild=False)\n",
    "        print(f\"âœ… Incremental Gold complete: 15min={rows15}, daily={rowsD}\")\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        f15 = self.spark.table(self.fact_15_fqn)\n",
    "        fd  = self.spark.table(self.fact_daily_fqn)\n",
    "        try:\n",
    "            parts = self.spark.sql(f\"SHOW PARTITIONS {self.fact_15_fqn}\").limit(5).collect()\n",
    "            part_preview = \", \".join([r[0] for r in parts]) if parts else \"(no partitions yet)\"\n",
    "        except Exception:\n",
    "            part_preview = \"(partitions not listed)\"\n",
    "        print(f\"ðŸ”Ž {self.fact_15_fqn}: {f15.count()} rows, partitions preview: {part_preview}\")\n",
    "        print(f\"ðŸ”Ž {self.fact_daily_fqn}: {fd.count()} rows\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Example usage (run as a notebook cell)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "GB = GoldBuilder(conf)\n",
    "# Option A: full rebuild (recommended after changes)\n",
    "GB.rebuild_all()\n",
    "\n",
    "# Option B: only new since a timestamp\n",
    "# GB.incremental_upsert(\"2025-05-01T00:00:00\")\n",
    "\n",
    "GB.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e5846a3-c97a-4313-9b53-b2fbc1f90055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------- orchestration ----------\n",
    "conf  = Config()\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "_bootstrap_uc(spark, conf.catalog, conf.db_name)\n",
    "\n",
    "silver_fact_fqn   = resolve_silver_fqn(conf)\n",
    "region_lookup_fqn = conf.table_fqn(conf.region_lookup)\n",
    "\n",
    "# targets\n",
    "t_region_hourly   = conf.table_fqn(\"traffic_gold_region_hourly\")\n",
    "t_detector_hourly = conf.table_fqn(\"traffic_gold_detector_hourly\")\n",
    "t_region_monthly  = conf.table_fqn(\"traffic_gold_region_monthly\")\n",
    "t_detector_cong   = conf.table_fqn(\"traffic_gold_detector_congestion\")\n",
    "t_gold_errors     = conf.table_fqn(\"traffic_gold_errors\")  # kept for parity (not used in MERGE path)\n",
    "\n",
    "ensure_region_lookup_coverage(spark, silver_fact_fqn, region_lookup_fqn, backfill_unknown=True)\n",
    "\n",
    "# precreate (first run) so MERGE targets exist\n",
    "precreate_gold_tables(\n",
    "    spark=spark,\n",
    "    silver_fact_fqn=silver_fact_fqn,\n",
    "    region_lookup_fqn=region_lookup_fqn,\n",
    "    t_region_hourly=t_region_hourly,\n",
    "    t_detector_hourly=t_detector_hourly,\n",
    "    t_region_monthly=t_region_monthly,\n",
    "    t_detector_cong=t_detector_cong,\n",
    "    dev_mode=DEV_MODE\n",
    ")\n",
    "\n",
    "# CDF streaming source from Silver (ignore deletes; use postimages)\n",
    "streaming_df = (\n",
    "    spark.readStream\n",
    "         .format(\"delta\")\n",
    "         .option(\"readChangeFeed\", \"true\")\n",
    "         .option(\"startingVersion\", CDF_FROM_VERSION)   # or .option(\"startingTimestamp\",\"2025-01-01\")\n",
    "         .table(silver_fact_fqn)\n",
    "         .where(col(\"_change_type\").isin(\"insert\",\"update_postimage\"))\n",
    ")\n",
    "\n",
    "checkpoint_dir = f\"{conf.checkpoint_base}/gold_cdf/{silver_fact_fqn.replace('.','_')}\"\n",
    "\n",
    "foreach_batch = make_foreach_batch_cdf_merge(\n",
    "    silver_fact_fqn=silver_fact_fqn,\n",
    "    region_lookup_fqn=region_lookup_fqn,\n",
    "    thresholds=THRESHOLDS,\n",
    "    t_region_hourly=t_region_hourly,\n",
    "    t_detector_hourly=t_detector_hourly,\n",
    "    t_region_monthly=t_region_monthly,\n",
    "    t_detector_cong=t_detector_cong\n",
    ")\n",
    "\n",
    "q = (streaming_df.writeStream\n",
    "     .foreachBatch(foreach_batch)\n",
    "     .option(\"checkpointLocation\", checkpoint_dir)\n",
    "     .outputMode(\"update\")   # foreachBatch ignores this for MERGE; harmless\n",
    "     .trigger(once=True)\n",
    "     .start())\n",
    "q.awaitTermination()\n",
    "\n",
    "# --- smoke/validate (optional) ---\n",
    "for t in [t_region_hourly, t_detector_hourly, t_region_monthly, t_detector_cong]:\n",
    "    try:\n",
    "        print(t, spark.table(t).count(), \"rows\")\n",
    "    except Exception as e:\n",
    "        print(\"Missing:\", t, e)\n",
    "print(\"âœ… gold (CDF + MERGE) completed.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4702320059868562,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "6_gold_loader.py",
   "widgets": {
    "ENV": {
     "currentValue": "dev",
     "nuid": "87ba2e25-d59e-414c-8236-2d4a0f1e7d22",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "qa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev",
        "qa"
       ]
      }
     }
    },
    "METASTORE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "7ef4ff7d-6700-4b73-94bc-c700e9465063",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "STORAGE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "73c9b908-604e-48b5-b129-1b8811889172",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
