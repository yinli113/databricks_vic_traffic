{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ffe0aa-7ef3-4ae8-baf7-84df83d022f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./1_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81024517-c083-4230-81bc-aa0355e0fd42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, importlib\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"ENV\", \"dev\", [\"dev\", \"qa\"], \"Environment\")\n",
    "    dbutils.widgets.dropdown(\"STORAGE_ACCOUNT\", \"trafficsa2\", [\"trafficsa2\", \"trafficsaqa\"], \"Storage account\")\n",
    "    dbutils.widgets.text(\"METASTORE_ACCOUNT\", \"trafficsa2\", \"Metastore account\")\n",
    "    os.environ[\"ENV\"] = dbutils.widgets.get(\"ENV\").strip().lower()\n",
    "    os.environ[\"STORAGE_ACCOUNT\"] = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\n",
    "    os.environ[\"METASTORE_ACCOUNT\"] = (dbutils.widgets.get(\"METASTORE_ACCOUNT\") or os.environ[\"STORAGE_ACCOUNT\"]).strip()\n",
    "except NameError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4af11ad3-a7de-4a50-ac01-7008061d358e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, SparkSession, DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "class GoldBuilder:\n",
    "    \"\"\"\n",
    "    Gold star schema built from Silver.\n",
    "      - Assumes Silver has: TimeKey (midnight, BIGINT), DetectorKey (BIGINT), RegionKey (BIGINT),\n",
    "        V00..V95, NB_SCATS_SITE, NB_DETECTOR, CT_RECORDS, QT_VOLUME_24HOUR, CT_ALARM_24HOUR.\n",
    "      - Melts V00..V95 to 15-min rows and computes INTERVAL TimeKey for dim_time + 15-min fact.\n",
    "      - Daily fact is derived from Silver's midnight TimeKey -> ReadingDate.\n",
    "      - dim_detector is built from Silver keys and enriched by dim_region (region_code/suburb).\n",
    "      - No SiteKey anywhere.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conf_obj: \"Config\" = None):\n",
    "        self.conf = conf_obj or conf\n",
    "        self.spark: SparkSession = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "        self.catalog = self.conf.catalog\n",
    "        self.db_name = self.conf.db_name\n",
    "\n",
    "        # FQNs\n",
    "        self.silver_fqn       = self.conf.table_fqn(self.conf.silver_table)\n",
    "        self.dim_time_fqn     = self.conf.table_fqn(\"dim_time\")\n",
    "        self.dim_detector_fqn = self.conf.table_fqn(\"dim_detector\")\n",
    "        self.dim_region_fqn   = self.conf.table_fqn(\"dim_region\")\n",
    "        self.fact_15_fqn      = self.conf.table_fqn(\"fact_traffic_15min\")\n",
    "        self.fact_daily_fqn   = self.conf.table_fqn(\"fact_daily_summary\")\n",
    "\n",
    "        self._bootstrap_uc()\n",
    "        self._ensure_dim_tables()\n",
    "        self._ensure_fact_tables()\n",
    "\n",
    "    # ---------------------------- bootstrap / DDL ----------------------------\n",
    "    def _bootstrap_uc(self):\n",
    "        self.spark.sql(f\"CREATE CATALOG IF NOT EXISTS {self.catalog}\")\n",
    "        self.spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS {self.catalog}.{self.db_name}\")\n",
    "        self.spark.sql(f\"USE CATALOG {self.catalog}\")\n",
    "        self.spark.sql(f\"USE {self.db_name}\")\n",
    "\n",
    "    def _ensure_dim_tables(self):\n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE {self.dim_time_fqn} (\n",
    "                TimeKey BIGINT,\n",
    "                Date DATE,\n",
    "                Hour INT,\n",
    "                Year INT,\n",
    "                Month INT,\n",
    "                DayOfWeek STRING,\n",
    "                WeekdayFlag BOOLEAN\n",
    "            ) USING DELTA\n",
    "        \"\"\")\n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE {self.dim_detector_fqn} (\n",
    "                DetectorKey BIGINT,\n",
    "                NB_SCATS_SITE INT,\n",
    "                NB_DETECTOR INT,\n",
    "                NM_REGION STRING,\n",
    "                RegionKey BIGINT,\n",
    "                suburb STRING\n",
    "            ) USING DELTA\n",
    "        \"\"\")\n",
    "\n",
    "    def _ensure_fact_tables(self):\n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE {self.fact_15_fqn} (\n",
    "                TimeKey BIGINT,\n",
    "                DetectorKey BIGINT,\n",
    "                RegionKey BIGINT,\n",
    "                IntervalStartTime TIMESTAMP,\n",
    "                Volume BIGINT,\n",
    "                Year INT,\n",
    "                Month INT\n",
    "            ) USING DELTA\n",
    "            PARTITIONED BY (Year, Month)\n",
    "        \"\"\")\n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE {self.fact_daily_fqn} (\n",
    "                DateKey INT,\n",
    "                DetectorKey BIGINT,\n",
    "                RegionKey BIGINT,\n",
    "                ReadingDate DATE,\n",
    "                TotalVolume BIGINT,\n",
    "                AlarmCount BIGINT,\n",
    "                IntervalsWithData INT\n",
    "            ) USING DELTA\n",
    "        \"\"\")\n",
    "\n",
    "    # ---------------------------- helpers ----------------------------\n",
    "    def _available_vcols(self, df: DataFrame):\n",
    "        return [f\"V{i:02d}\" for i in range(96) if f\"V{i:02d}\" in df.columns]\n",
    "\n",
    "    # Wide (V00..V95) -> long with IntervalStartTime and interval TimeKey (computed later)\n",
    "    def _melt_15min(self, df: DataFrame) -> DataFrame:\n",
    "        vcols = self._available_vcols(df)\n",
    "        if not vcols:\n",
    "            raise ValueError(\"No V00..V95 columns found in Silver. Rebuild Silver so it includes the volume columns.\")\n",
    "\n",
    "        base_cols = [\"DetectorKey\", \"NB_SCATS_SITE\", \"NB_DETECTOR\", \"RegionKey\", \"TimeKey\"]\n",
    "        sel_cols = [c for c in base_cols if c in df.columns] + vcols\n",
    "\n",
    "        df2 = (df.select(*sel_cols)\n",
    "                 .withColumn(\"volumes\", F.array(*[F.col(c).cast(\"long\") for c in vcols])))\n",
    "\n",
    "        exploded = df2.select(\n",
    "            *[c for c in base_cols if c in df2.columns],\n",
    "            F.posexplode_outer(\"volumes\").alias(\"IntervalIndex\", \"Volume\")\n",
    "        )\n",
    "\n",
    "        # Silver.TimeKey is midnight for the day (epoch seconds). Build interval start timestamps.\n",
    "        exploded = (exploded\n",
    "            .withColumn(\"MidnightTs\", F.to_timestamp(F.from_unixtime(F.col(\"TimeKey\"))))\n",
    "            .withColumn(\"IntervalStartTime\", F.expr(\"MidnightTs + make_interval(0,0,0,0,0, IntervalIndex*15, 0)\"))\n",
    "            .drop(\"MidnightTs\"))\n",
    "\n",
    "        return exploded\n",
    "\n",
    "    # ---------------------------- dimensions ----------------------------\n",
    "    # Use INTERVAL time (not Silver midnight) for dim_time PK\n",
    "    def _upsert_dim_time(self, intervals_df: DataFrame):\n",
    "        dim = (intervals_df\n",
    "               .withColumn(\"IntervalTimeKey\", F.col(\"IntervalStartTime\").cast(\"long\").cast(\"bigint\"))\n",
    "               .select(\n",
    "                   F.col(\"IntervalTimeKey\").alias(\"TimeKey\"),\n",
    "                   F.to_date(\"IntervalStartTime\").alias(\"Date\"),\n",
    "                   F.hour(\"IntervalStartTime\").alias(\"Hour\"),\n",
    "                   F.year(\"IntervalStartTime\").alias(\"Year\"),\n",
    "                   F.month(\"IntervalStartTime\").alias(\"Month\"),\n",
    "                   F.date_format(\"IntervalStartTime\", \"E\").alias(\"DayOfWeek\"),\n",
    "                   F.when(F.dayofweek(\"IntervalStartTime\").isin(1, 7), F.lit(False))\n",
    "                    .otherwise(F.lit(True)).alias(\"WeekdayFlag\")\n",
    "               )\n",
    "               .dropDuplicates([\"TimeKey\"])\n",
    "        )\n",
    "        dim.createOrReplaceTempView(\"__dim_time_src\")\n",
    "\n",
    "        self.spark.sql(f\"\"\"\n",
    "            MERGE INTO {self.dim_time_fqn} AS tgt\n",
    "            USING __dim_time_src AS src\n",
    "              ON tgt.TimeKey = src.TimeKey\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "              tgt.Date = src.Date,\n",
    "              tgt.Hour = src.Hour,\n",
    "              tgt.Year = src.Year,\n",
    "              tgt.Month = src.Month,\n",
    "              tgt.DayOfWeek = src.DayOfWeek,\n",
    "              tgt.WeekdayFlag = src.WeekdayFlag\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "              TimeKey, Date, Hour, Year, Month, DayOfWeek, WeekdayFlag\n",
    "            ) VALUES (\n",
    "              src.TimeKey, src.Date, src.Hour, src.Year, src.Month, src.DayOfWeek, src.WeekdayFlag\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "    # Build from Silver keys, enrich from dim_region; avoid NM_REGION dependency in Silver\n",
    "    def _upsert_dim_detector(self, silver_df: DataFrame):\n",
    "        dim = (silver_df\n",
    "               .select(\"DetectorKey\", \"NB_SCATS_SITE\", \"NB_DETECTOR\", \"RegionKey\")\n",
    "               .dropna(subset=[\"DetectorKey\", \"NB_SCATS_SITE\", \"NB_DETECTOR\"])\n",
    "               .dropDuplicates())\n",
    "\n",
    "        try:\n",
    "            region = self.spark.table(self.dim_region_fqn).select(\"RegionKey\", \"region_code\", \"suburb\")\n",
    "            dim = dim.join(region, \"RegionKey\", \"left\")\n",
    "        except AnalysisException:\n",
    "            dim = dim.withColumn(\"region_code\", F.lit(None).cast(\"string\")) \\\n",
    "                     .withColumn(\"suburb\", F.lit(None).cast(\"string\"))\n",
    "\n",
    "        # Keep NM_REGION column for compatibility, populated from region_code\n",
    "        dim = dim.withColumn(\"NM_REGION\", F.col(\"region_code\")).drop(\"region_code\")\n",
    "\n",
    "        dim.createOrReplaceTempView(\"__dim_detector_src\")\n",
    "\n",
    "        self.spark.sql(f\"\"\"\n",
    "            MERGE INTO {self.dim_detector_fqn} AS tgt\n",
    "            USING __dim_detector_src AS src\n",
    "              ON tgt.DetectorKey = src.DetectorKey\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "              tgt.NB_SCATS_SITE = src.NB_SCATS_SITE,\n",
    "              tgt.NB_DETECTOR   = src.NB_DETECTOR,\n",
    "              tgt.NM_REGION     = src.NM_REGION,\n",
    "              tgt.RegionKey     = src.RegionKey,\n",
    "              tgt.suburb        = src.suburb\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "              DetectorKey, NB_SCATS_SITE, NB_DETECTOR, NM_REGION, RegionKey, suburb\n",
    "            ) VALUES (\n",
    "              src.DetectorKey, src.NB_SCATS_SITE, src.NB_DETECTOR, src.NM_REGION, src.RegionKey, src.suburb\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "    # ---------------------------- facts ----------------------------\n",
    "    def _build_fact_15min(self, silver_df: DataFrame):\n",
    "        melted = self._melt_15min(silver_df).cache()\n",
    "\n",
    "        fact = (melted\n",
    "                .withColumn(\"TimeKey\", F.col(\"IntervalStartTime\").cast(\"long\").cast(\"bigint\"))\n",
    "                .withColumn(\"Year\", F.year(\"IntervalStartTime\"))\n",
    "                .withColumn(\"Month\", F.month(\"IntervalStartTime\"))\n",
    "                .select(\"TimeKey\", \"DetectorKey\", \"RegionKey\", \"IntervalStartTime\", \"Volume\", \"Year\", \"Month\"))\n",
    "\n",
    "        self.spark.sql(f\"TRUNCATE TABLE {self.fact_15_fqn}\")\n",
    "        (fact.write\n",
    "             .mode(\"append\")\n",
    "             .format(\"delta\")\n",
    "             .partitionBy(\"Year\", \"Month\")\n",
    "             .saveAsTable(self.fact_15_fqn))\n",
    "\n",
    "        rows = fact.count()\n",
    "        melted.unpersist()\n",
    "        return rows\n",
    "\n",
    "    def _build_fact_daily(self, silver_df: DataFrame):\n",
    "        # Silver.TimeKey is midnight; derive day directly from it\n",
    "        fact = (silver_df\n",
    "                .withColumn(\"ReadingDate\", F.to_date(F.from_unixtime(F.col(\"TimeKey\"))))\n",
    "                .select(\n",
    "                    F.date_format(\"ReadingDate\", \"yyyyMMdd\").cast(\"int\").alias(\"DateKey\"),\n",
    "                    \"DetectorKey\",\n",
    "                    \"RegionKey\",\n",
    "                    \"ReadingDate\",\n",
    "                    F.col(\"QT_VOLUME_24HOUR\").cast(\"long\").alias(\"TotalVolume\"),\n",
    "                    F.col(\"CT_ALARM_24HOUR\").cast(\"long\").alias(\"AlarmCount\"),\n",
    "                    F.col(\"CT_RECORDS\").cast(\"int\").alias(\"IntervalsWithData\")\n",
    "                ))\n",
    "\n",
    "        self.spark.sql(f\"TRUNCATE TABLE {self.fact_daily_fqn}\")\n",
    "        fact.write.mode(\"append\").format(\"delta\").saveAsTable(self.fact_daily_fqn)\n",
    "        return fact.count()\n",
    "\n",
    "    # ---------------------------- public APIs ----------------------------\n",
    "    def rebuild_all(self):\n",
    "        # Source\n",
    "        silver = self.spark.table(self.silver_fqn)\n",
    "\n",
    "        # Build dims\n",
    "        melted = self._melt_15min(silver)\n",
    "        self._upsert_dim_time(melted)\n",
    "        self._upsert_dim_detector(silver)\n",
    "\n",
    "        # Build facts\n",
    "        rows15 = self._build_fact_15min(silver)\n",
    "        rowsD  = self._build_fact_daily(silver)\n",
    "\n",
    "        print(f\"✅ Full Gold rebuild complete: 15min={rows15}, daily={rowsD}\")\n",
    "\n",
    "    def validate(self):\n",
    "        f15 = self.spark.table(self.fact_15_fqn)\n",
    "        fd  = self.spark.table(self.fact_daily_fqn)\n",
    "        print(f\"🔎 {self.fact_15_fqn}: {f15.count()} rows\")\n",
    "        try:\n",
    "            parts = self.spark.sql(f\"SHOW PARTITIONS {self.fact_15_fqn}\").collect()\n",
    "            if parts:\n",
    "                sample = \", \".join([r['partition'] for r in parts[:5]] + ([\"...\"] if len(parts) > 5 else []))\n",
    "                print(\"   partitions example:\", sample)\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(f\"🔎 {self.fact_daily_fqn}: {fd.count()} rows\")\n",
    "\n",
    "\n",
    "# ---------------------------- example run ----------------------------\n",
    "GB = GoldBuilder(conf)\n",
    "GB.rebuild_all()\n",
    "GB.validate()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4702320059868562,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "6_gold_loader.py",
   "widgets": {
    "ENV": {
     "currentValue": "dev",
     "nuid": "87ba2e25-d59e-414c-8236-2d4a0f1e7d22",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "qa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev",
        "qa"
       ]
      }
     }
    },
    "METASTORE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "7ef4ff7d-6700-4b73-94bc-c700e9465063",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "trafficsa2",
      "label": "Metastore account",
      "name": "METASTORE_ACCOUNT",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "STORAGE_ACCOUNT": {
     "currentValue": "trafficsa2",
     "nuid": "73c9b908-604e-48b5-b129-1b8811889172",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "trafficsa2",
      "label": "Storage account",
      "name": "STORAGE_ACCOUNT",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "trafficsa2",
        "trafficsaqa"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
