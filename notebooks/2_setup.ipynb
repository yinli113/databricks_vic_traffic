{"cells": [{"cell_type": "code", "execution_count": 0, "metadata": {"application/vnd.databricks.v1+cell": {"cellMetadata": {"byteLimit": 2048000, "rowLimit": 10000}, "inputWidgets": {}, "nuid": "5926f8a1-9be9-4130-bfaa-d596c5579673", "showTitle": false, "tableResultSettingsMap": {}, "title": ""}}, "outputs": [], "source": ["%run ./1_config.py"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"application/vnd.databricks.v1+cell": {"cellMetadata": {"byteLimit": 2048000, "rowLimit": 10000}, "inputWidgets": {}, "nuid": "e85ce501-e10a-44f5-8d33-cc125bf61d37", "showTitle": false, "tableResultSettingsMap": {}, "title": ""}}, "outputs": [], "source": ["# --- inline params (dev/qa) — optional; keep if running this as a notebook ---\n", "import os, importlib\n", "try:\n", "    dbutils.widgets.dropdown(\"ENV\", \"dev\", [\"dev\", \"qa\"], \"Environment\")\n", "    dbutils.widgets.dropdown(\"STORAGE_ACCOUNT\", \"trafficsa2\", [\"trafficsa2\", \"trafficsaqa\"], \"Storage account\")\n", "    dbutils.widgets.text(\"METASTORE_ACCOUNT\", \"trafficsa2\", \"Metastore account\")\n", "    os.environ[\"ENV\"] = dbutils.widgets.get(\"ENV\").strip().lower()\n", "    os.environ[\"STORAGE_ACCOUNT\"] = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\n", "    os.environ[\"METASTORE_ACCOUNT\"] = (dbutils.widgets.get(\"METASTORE_ACCOUNT\") or os.environ[\"STORAGE_ACCOUNT\"]).strip()\n", "except NameError:\n", "    # running as a pure module; ENV already set or defaults used\n", "    pass\n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"application/vnd.databricks.v1+cell": {"cellMetadata": {}, "inputWidgets": {}, "nuid": "520d3d49-1254-4c26-b3b5-bc6e1efcd631", "showTitle": false, "tableResultSettingsMap": {}, "title": ""}}, "outputs": [], "source": ["import os, time\n", "from pyspark.sql import functions as F\n", "\n", "class SetupHelper:\n", "    def __init__(self):\n", "        self.catalog = conf.catalog\n", "        self.db_name = conf.db_name\n", "\n", "    # ------------------ catalog & db ------------------\n", "    def create_catalog_and_db(self):\n", "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS {self.catalog}\")\n", "        spark.sql(f\"USE CATALOG {self.catalog}\")\n", "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.db_name}\")\n", "        spark.sql(f\"USE {self.db_name}\")\n", "\n", "    # ------------------ tables ------------------\n", "    def create_tables(self):\n", "        spark.sql(f\"USE CATALOG {self.catalog}\")\n", "        spark.sql(f\"USE {self.db_name}\")\n", "\n", "        # --- drop old/deprecated ---\n", "        spark.sql(f\"DROP TABLE IF EXISTS {self.db_name}.dim_site\")    # no longer needed\n", "        spark.sql(f\"DROP TABLE IF EXISTS {self.db_name}.dim_region\")  # force recreate\n", "\n", "        # --- dim_region ---\n", "        spark.sql(f\"\"\"\n", "            CREATE TABLE {self.db_name}.dim_region (\n", "                RegionKey   BIGINT NOT NULL,\n", "                region_code STRING NOT NULL,\n", "                suburb      STRING\n", "            )\n", "            USING DELTA\n", "        \"\"\")\n", "\n", "        # --- dim_time ---\n", "        spark.sql(f\"\"\"\n", "            CREATE TABLE IF NOT EXISTS {self.db_name}.dim_time (\n", "                TimeKey     BIGINT  NOT NULL,\n", "                Date        DATE    NOT NULL,\n", "                Hour        INT     NOT NULL,\n", "                Year        INT     NOT NULL,\n", "                Month       INT     NOT NULL,\n", "                DayOfWeek   STRING  NOT NULL,\n", "                WeekdayFlag BOOLEAN NOT NULL,\n", "                PRIMARY KEY (TimeKey)\n", "            )\n", "            USING DELTA\n", "        \"\"\")\n", "\n", "        # --- dim_detector ---\n", "        spark.sql(f\"\"\"\n", "            CREATE TABLE IF NOT EXISTS {self.db_name}.dim_detector (\n", "                DetectorKey   BIGINT  NOT NULL,\n", "                NB_SCATS_SITE INT     NOT NULL,\n", "                NB_DETECTOR   INT     NOT NULL,\n", "                NM_REGION     STRING,\n", "                RegionKey     BIGINT,\n", "                suburb        STRING,\n", "                PRIMARY KEY (DetectorKey)\n", "            )\n", "            USING DELTA\n", "        \"\"\")\n", "\n", "        # --- fact_traffic_15min ---\n", "        spark.sql(f\"\"\"\n", "            CREATE TABLE IF NOT EXISTS {self.db_name}.fact_traffic_15min (\n", "                TimeKey          BIGINT NOT NULL,\n", "                DetectorKey      BIGINT NOT NULL,\n", "                RegionKey        BIGINT NOT NULL,\n", "                IntervalStartTime TIMESTAMP NOT NULL,\n", "                volume_map       MAP<STRING, INT>,\n", "                CT_RECORDS       INT,\n", "                QT_VOLUME_24HOUR INT,\n", "                CT_ALARM_24HOUR  INT\n", "            )\n", "            USING DELTA\n", "        \"\"\")\n", "\n", "        # --- fact_daily_summary ---\n", "        spark.sql(f\"\"\"\n", "            CREATE TABLE IF NOT EXISTS {self.db_name}.fact_daily_summary (\n", "                TimeKey       BIGINT NOT NULL,\n", "                DetectorKey   BIGINT NOT NULL,\n", "                RegionKey     BIGINT NOT NULL,\n", "                ReadingDate   DATE NOT NULL,\n", "                records_24h   INT,\n", "                volume_24h    INT,\n", "                alarms_24h    INT\n", "            )\n", "            USING DELTA\n", "        \"\"\")\n", "\n", "    # ------------------ seed region ------------------\n", "    def seed_dim_region_from_map(self):\n", "        items = [(code, name) for code, name in REGION_MAP.items()]\n", "        df = (spark.createDataFrame(items, \"region_code STRING, suburb STRING\")\n", "              .withColumn(\"RegionKey\", F.crc32(F.upper(F.col(\"region_code\"))).cast(\"bigint\"))\n", "              .select(\"RegionKey\", \"region_code\", \"suburb\"))\n", "\n", "        (df.write\n", "            .format(\"delta\")\n", "            .mode(\"overwrite\")            # always overwrite to keep consistent\n", "            .option(\"overwriteSchema\", \"true\")\n", "            .saveAsTable(f\"{self.db_name}.dim_region\"))\n", "\n", "    # ------------------ utils ------------------\n", "    def assert_table(self, name):\n", "        spark.sql(f\"DESCRIBE TABLE {self.db_name}.{name}\").collect()\n", "\n", "    def setup(self):\n", "        t0 = time.time()\n", "        self.create_catalog_and_db()\n", "        self.create_tables()\n", "        self.seed_dim_region_from_map()\n", "        print(f\"✅ Setup completed in {int(time.time() - t0)}s\")\n", "\n", "    def validate(self):\n", "        t0 = time.time()\n", "        for tbl in [\n", "            \"dim_time\",\n", "            \"dim_detector\",\n", "            \"dim_region\",\n", "            \"fact_traffic_15min\",\n", "            \"fact_daily_summary\"\n", "        ]:\n", "            self.assert_table(tbl)\n", "        print(f\"✅ Validation completed in {int(time.time() - t0)}s\")\n", "\n", "\n", "# ------------------ run setup ------------------\n", "setup = SetupHelper()\n", "setup.setup()\n", "setup.validate()"]}], "metadata": {"application/vnd.databricks.v1+notebook": {"computePreferences": {"hardware": {"accelerator": null, "gpuPoolId": null, "memory": null}}, "dashboards": [], "environmentMetadata": {"base_environment": "", "environment_version": "2"}, "inputWidgetPreferences": null, "language": "python", "notebookMetadata": {"mostRecentlyExecutedCommandWithImplicitDF": {"commandId": -1, "dataframes": ["_sqldf"]}, "pythonIndentUnit": 4}, "notebookName": "2_setup.py", "widgets": {"ENV": {"currentValue": "dev", "nuid": "25fe948a-0971-4427-a75e-03ce72532097", "typedWidgetInfo": {"autoCreated": false, "defaultValue": "dev", "label": "Environment", "name": "ENV", "options": {"widgetDisplayType": "Dropdown", "choices": ["dev", "qa"], "fixedDomain": true, "multiselect": false}, "parameterDataType": "String"}, "widgetInfo": {"widgetType": "dropdown", "defaultValue": "dev", "label": "Environment", "name": "ENV", "options": {"widgetType": "dropdown", "autoCreated": null, "choices": ["dev", "qa"]}}}, "METASTORE_ACCOUNT": {"currentValue": "trafficsa2", "nuid": "46458228-d524-448c-9a03-3dc264abeb01", "typedWidgetInfo": {"autoCreated": false, "defaultValue": "trafficsa2", "label": "Metastore account", "name": "METASTORE_ACCOUNT", "options": {"widgetDisplayType": "Text", "validationRegex": null}, "parameterDataType": "String"}, "widgetInfo": {"widgetType": "text", "defaultValue": "trafficsa2", "label": "Metastore account", "name": "METASTORE_ACCOUNT", "options": {"widgetType": "text", "autoCreated": null, "validationRegex": null}}}, "STORAGE_ACCOUNT": {"currentValue": "trafficsa2", "nuid": "40dfb085-956e-4a9c-a0f9-83fa733fbb01", "typedWidgetInfo": {"autoCreated": false, "defaultValue": "trafficsa2", "label": "Storage account", "name": "STORAGE_ACCOUNT", "options": {"widgetDisplayType": "Dropdown", "choices": ["trafficsa2", "trafficsaqa"], "fixedDomain": true, "multiselect": false}, "parameterDataType": "String"}, "widgetInfo": {"widgetType": "dropdown", "defaultValue": "trafficsa2", "label": "Storage account", "name": "STORAGE_ACCOUNT", "options": {"widgetType": "dropdown", "autoCreated": null, "choices": ["trafficsa2", "trafficsaqa"]}}}}}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 0}