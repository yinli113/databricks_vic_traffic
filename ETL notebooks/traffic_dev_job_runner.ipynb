{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e009195-0c78-4ba4-91b0-26b74b9e7948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " Databricks notebook: traffic_dev_job_runner\n",
    "\n",
    "# ── Widgets (tweak then Run All) ────────────────────────────────────────────────\n",
    "dbutils.widgets.text(\"env\", \"dev\")\n",
    "dbutils.widgets.text(\"batch_start\", \"2025-01-01\")\n",
    "dbutils.widgets.text(\"batch_end\", \"2025-05-31\")\n",
    "dbutils.widgets.text(\"stream_pattern\", \"VSDATA_202506*.csv\")\n",
    "\n",
    "dbutils.widgets.dropdown(\"do_setup\", \"true\", [\"true\", \"false\"])\n",
    "dbutils.widgets.dropdown(\"do_bronze_batch\", \"true\", [\"true\", \"false\"])\n",
    "dbutils.widgets.dropdown(\"do_bronze_stream\", \"true\", [\"true\", \"false\"])\n",
    "dbutils.widgets.dropdown(\"do_silver\", \"true\", [\"true\", \"false\"])\n",
    "dbutils.widgets.dropdown(\"do_gold\", \"true\", [\"true\", \"false\"])\n",
    "\n",
    "# ── Read widget values ─────────────────────────────────────────────────────────\n",
    "env                = dbutils.widgets.get(\"env\")\n",
    "batch_start        = dbutils.widgets.get(\"batch_start\")\n",
    "batch_end          = dbutils.widgets.get(\"batch_end\")\n",
    "stream_pattern     = dbutils.widgets.get(\"stream_pattern\")\n",
    "\n",
    "do_setup           = dbutils.widgets.get(\"do_setup\") == \"true\"\n",
    "do_bronze_batch    = dbutils.widgets.get(\"do_bronze_batch\") == \"true\"\n",
    "do_bronze_stream   = dbutils.widgets.get(\"do_bronze_stream\") == \"true\"\n",
    "do_silver          = dbutils.widgets.get(\"do_silver\") == \"true\"\n",
    "do_gold            = dbutils.widgets.get(\"do_gold\") == \"true\"\n",
    "\n",
    "# ── Imports from your modules (paths unchanged) ────────────────────────────────\n",
    "from config import conf, Config\n",
    "from setup import SetupHelper\n",
    "from bronze_loader import LoadRawTraffic\n",
    "from silver_loader import SilverLoader, create_region_lookup\n",
    "from gold_loader import make_foreach_batch as make_gold_foreach_batch\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Ensure we operate in the selected catalog/db\n",
    "spark.sql(f\"USE CATALOG {conf.catalog}\")\n",
    "spark.sql(f\"USE {conf.db_name}\")\n",
    "\n",
    "# ── Helpers ───────────────────────────────────────────────────────────────────\n",
    "import time\n",
    "\n",
    "def t():\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def banner(msg):\n",
    "    print(f\"\\n{'='*80}\\n[{t()}] {msg}\\n{'='*80}\")\n",
    "\n",
    "# ── 1) SETUP (create DB and empty tables) ─────────────────────────────────────\n",
    "if do_setup:\n",
    "    banner(\"SETUP: creating/validating database and base tables\")\n",
    "    sh = SetupHelper(catalog=conf.catalog)\n",
    "    sh.setup()\n",
    "    sh.validate()\n",
    "\n",
    "# ── 2) BRONZE LOAD (batch + optional stream) ──────────────────────────────────\n",
    "if do_bronze_batch or do_bronze_stream:\n",
    "    banner(\"BRONZE: loading raw CSVs into bronze table\")\n",
    "    bronze = LoadRawTraffic(catalog=conf.catalog, table_name=conf.bronze_table)\n",
    "\n",
    "    # Make sure database exists in current session (idempotent)\n",
    "    bronze.create_db()\n",
    "\n",
    "    if do_bronze_batch:\n",
    "        print(f\"[{t()}] Bronze BATCH load: {batch_start} → {batch_end}\")\n",
    "        bronze.batch_load(start_date=batch_start, end_date=batch_end)\n",
    "\n",
    "    if do_bronze_stream:\n",
    "        print(f\"[{t()}] Bronze STREAM load pattern: {stream_pattern}\")\n",
    "        bronze.stream_load(file_pattern=stream_pattern)\n",
    "\n",
    "    bronze.validate_table()\n",
    "\n",
    "# ── 3) SILVER (streaming foreachBatch transform) ──────────────────────────────\n",
    "if do_silver:\n",
    "    banner(\"SILVER: unpivot + dim tables via foreachBatch (trigger=once)\")\n",
    "    # Create region lookup once (idempotent overwrite)\n",
    "    create_region_lookup(spark)\n",
    "\n",
    "    bronze_table_fqn = conf.table_fqn(conf.bronze_table)\n",
    "    silver_table_fqn = conf.table_fqn(conf.silver_table)\n",
    "    region_lookup_fqn = conf.table_fqn(conf.region_lookup)\n",
    "    silver_checkpoint = f\"{conf.checkpoint_base}/silver/{conf.silver_table}\"\n",
    "\n",
    "    streaming_df = spark.readStream.format(\"delta\").table(bronze_table_fqn)\n",
    "    foreach_batch_fn = SilverLoader.make_foreach_batch(\n",
    "        silver_table_fqn,\n",
    "        region_lookup_fqn\n",
    "    )\n",
    "\n",
    "    q_silver = (\n",
    "        streaming_df.writeStream\n",
    "        .foreachBatch(foreach_batch_fn)\n",
    "        .option(\"checkpointLocation\", silver_checkpoint)\n",
    "        .outputMode(\"append\")\n",
    "        .trigger(once=True)\n",
    "        .start()\n",
    "    )\n",
    "    q_silver.awaitTermination()\n",
    "    print(f\"[{t()}] SILVER complete.\")\n",
    "\n",
    "# ── 4) GOLD (streaming foreachBatch aggregates) ───────────────────────────────\n",
    "if do_gold:\n",
    "    banner(\"GOLD: rollups via foreachBatch (trigger=once)\")\n",
    "    silver_table_fqn = conf.table_fqn(conf.silver_table)\n",
    "    region_lookup_fqn = conf.table_fqn(conf.region_lookup)\n",
    "    # Keep your original gold checkpoint base on /tmp\n",
    "    gold_checkpoint = f\"/tmp/checkpoints/gold/{conf.silver_table}\"\n",
    "\n",
    "    streaming_df = spark.readStream.format(\"delta\").table(silver_table_fqn)\n",
    "    foreach_batch_gold = make_gold_foreach_batch(\n",
    "        conf.catalog,\n",
    "        conf.db_name,\n",
    "        region_lookup_fqn\n",
    "    )\n",
    "\n",
    "    q_gold = (\n",
    "        streaming_df.writeStream\n",
    "        .foreachBatch(foreach_batch_gold)\n",
    "        .option(\"checkpointLocation\", gold_checkpoint)\n",
    "        .outputMode(\"append\")\n",
    "        .trigger(once=True)\n",
    "        .start()\n",
    "    )\n",
    "    q_gold.awaitTermination()\n",
    "    print(f\"[{t()}] GOLD complete.\")\n",
    "\n",
    "# ── 5) Summary ────────────────────────────────────────────────────────────────\n",
    "banner(\"SUMMARY COUNTS\")\n",
    "\n",
    "def safe_count(tn):\n",
    "    try:\n",
    "        return spark.table(tn).count()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ {e}\"\n",
    "\n",
    "print(f\"Bronze: {conf.table_fqn(conf.bronze_table)} -> {safe_count(conf.table_fqn(conf.bronze_table))}\")\n",
    "print(f\"Silver: {conf.table_fqn(conf.silver_table)} -> {safe_count(conf.table_fqn(conf.silver_table))}\")\n",
    "for tn in [\n",
    "    \"traffic_gold_region_hourly\",\n",
    "    \"traffic_gold_detector_hourly\",\n",
    "    \"traffic_gold_region_monthly\",\n",
    "    \"traffic_gold_detector_congestion\",\n",
    "]:\n",
    "    fqn = conf.table_fqn(tn)\n",
    "    print(f\"Gold:   {fqn} -> {safe_count(fqn)}\")\n",
    "\n",
    "dbutils.notebook.exit(\"✅ Job finished\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "traffic_dev_job_runner",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
